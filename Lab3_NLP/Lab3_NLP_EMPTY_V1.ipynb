{"cells":[{"cell_type":"markdown","metadata":{"id":"LamUhQgk1s71"},"source":["# Lab: N-gram Language Models\n","\n","## Objectives\n","In this lab, you will learn about:\n","- N-gram language models and probability calculations\n","- Various smoothing techniques\n","- Sentence generation using n-gram models\n","- Model evaluation using perplexity\n","- Working with Arabic texts.\n"]},{"cell_type":"markdown","metadata":{"id":"38AWaqFQ1s75"},"source":["# Exercise 1: Exploring N-gram Language Models\n","\n","In this exercise, we will delve into the fundamentals of n-gram language models, a crucial component in natural language processing. N-grams are sequences of words (or tokens) that provide insight into the structure and patterns of language. By analyzing n-grams, we can gain valuable statistics about word occurrences and relationships, which are essential for various applications, including text generation, sentiment analysis, and machine translation.\n","\n","## Understanding N-grams and Context\n","\n","An **n-gram** is defined as a contiguous sequence of \\( n \\) items (typically words) from a given text.\n","\n","**Context** refers to the preceding words that provide information about the likelihood of a given word occurring after them.\n","\n","For example, in a bigram $ (w_{i-1}, w_i) $, the context is $ w_{i-1} $. The probability of $ w_i $ given the context $ w_{i-1} $ can be expressed as:\n","\n","$$\n","P(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}\n","$$\n","\n","where:\n","- $ C(w_{i-1}, w_i) $ is the count of the bigram $ (w_{i-1}, w_i) $.\n","- $ C(w_{i-1}) $ is the count of the context word $ w_{i-1} $.\n","\n","\n","For trigrams $ (w_{i-2}, w_{i-1}, w_i) $, the context is $ (w_{i-2}, w_{i-1}) $, and the probability can be expressed as:\n","\n","$$\n","P(w_i \\mid w_{i-2}, w_{i-1}) = \\frac{C(w_{i-2}, w_{i-1}, w_i)}{C(w_{i-2}, w_{i-1})}\n","$$\n","\n","where:\n","- $ C(w_{i-2}, w_{i-1}, w_i) $ is the count of the trigram $ (w_{i-2}, w_{i-1}, w_i) $.\n","- $ C(w_{i-2}, w_{i-1}) $ is the count of the context bigram $ (w_{i-2}, w_{i-1}) $.\n","\n","\n","\n","This formula provides a way to calculate the conditional probability of a word occurring based on its preceding context, which is critical for modeling language.\n","\n","## Questions\n","\n","### Question 1\n","Write a Python function that takes a set of sentences as input and calculates the statistics of the n-grams (both bigrams and trigrams) along with their respective context counts.\n","\n","### Question 2\n","Using the function you wrote in Question 1, compare the probabilities of the following sentences using both bigram and trigram models:\n","1. \"I enjoy Chocolate cake\"\n","2. \"I hate Chocolate cake\"\n","3. \"He told me about it\"\n","\n","Calculate the probabilities for both sentences using the respective models. What observations can you make about the resulting probabilities? Is there any problems?\n","\n","### Question 3\n","For unseen n-grams, assign a small probability of **0.0001** and repeat your calculations. What observations can you make about the resulting probabilities?\n","\n","### Question 4\n","Instead of calculating the probabilities directly, use the log probabilities, applying the relation:\n","\n","$$\n","\\log(P1 \\times P2 \\times P3) = \\log(P1) + \\log(P2) + \\log(P3)\n","$$\n","\n","Recalculate the log probabilities of sentences. What do you notice?\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PYUmxZBz1s76","outputId":"a5ccf451-6c69-426c-b111-1f5b4dd20d5d"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/wafaa/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/wafaa/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/share/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 152\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_sentence_probability\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m bigram_counts, bigram_contexts \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_ngram_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m trigram_counts, trigram_contexts \u001b[38;5;241m=\u001b[39m calculate_ngram_probabilities(corpus, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Example sentences\u001b[39;00m\n","Cell \u001b[0;32mIn[1], line 60\u001b[0m, in \u001b[0;36mcalculate_ngram_probabilities\u001b[0;34m(sentences, n)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_ngram_probabilities\u001b[39m(sentences, n):\n\u001b[1;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Calculate n-gram probabilities from sentences.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m      2. (n-1)-gram context counts (Counter object).\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     processed_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Count n-grams and (n-1)-grams\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     ngram_counts \u001b[38;5;241m=\u001b[39m Counter()\n","Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mprepare_sentences\u001b[0;34m(sentences, n)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#your code here\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 40\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     42\u001b[0m         processed\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m tokens \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/wafaa/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/share/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}],"source":["import nltk\n","from nltk import bigrams, trigrams\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","import math\n","\n","# Download required NLTK data\n","nltk.download('punkt')\n","\n","# Define the corpus (a collection of sentences)\n","corpus = [\n","    \"I love chocolate cake\",\n","    \"I love chocolate ice cream\",\n","    \"I love cookies\",\n","    \"I enjoy vanilla cake\",\n","    \"I enjoy vanilla ice cream\",\n","    \"I enjoy cookies\",\n","    \"Chocolate is my favorite\",\n","    \"Vanilla cookies are great\",\n","    \"Chocolate cookies are also great\"\n","]\n","\n","# ---- Question 1: Calculate n-gram statistics ----\n","\n","def prepare_sentences(sentences, n):\n","    \"\"\"\n","    Prepare sentences by adding start and end tokens <s> and <\\s>.\n","\n","    Input:\n","    - sentences: A list of sentences (strings).\n","    - n: The n-gram size (2 for bigrams, 3 for trigrams).\n","\n","    Output:\n","    - A list of tokenized sentences with added start and end tokens.\n","    \"\"\"\n","    processed = []\n","\n","    #your code here\n","    for sentence in sentences:\n","        tokens = word_tokenize(sentence.lower())\n","        if n==2:\n","            processed.append(['<s>']+ tokens + ['</s>'])\n","        elif n== 3:\n","            processed.append(['<s>', '<s>']+ tokens + ['</s>'])\n","            \n","    return processed\n","\n","def calculate_ngram_probabilities(sentences, n):\n","    \"\"\"\n","    Calculate n-gram probabilities from sentences.\n","\n","    Input:\n","    - sentences: A list of sentences (strings).\n","    - n: The n-gram size (2 for bigrams, 3 for trigrams).\n","\n","    Output:\n","      1. n-gram counts (Counter object).\n","      2. (n-1)-gram context counts (Counter object).\n","    \"\"\"\n","    processed_sentences = prepare_sentences(sentences, n)\n","\n","    # Count n-grams and (n-1)-grams\n","    ngram_counts = Counter()\n","    context_counts = Counter()\n","\n","    #your code here\n","    for sentence in processed_sentences:\n","        if n==2:\n","            ngrams = list(bigrams(sentence))\n","            contexts = [ng[0] for ng in ngrams]\n","        elif n==3:\n","            ngrams = list(trigrams(sentence))\n","            contexts = [(ng[0], ng[1]) for ng in ngrams]\n","            \n","        ngram_counts.update(ngrams)\n","        context_counts.update(contexts)\n","            \n","    return ngram_counts, context_counts\n","\n","\n","# ---- Question 2 & 3 & 4: Calculate sentence probability ----\n","\n","def calculate_sentence_probability(sentence, ngram_counts, context_counts, n):\n","    \"\"\"\n","    Calculate the probability of a sentence using an n-gram model without smoothing.\n","\n","    Input:\n","    - sentence: A sentence (string).\n","    - ngram_counts: Counts of n-grams (Counter object).\n","    - context_counts: Counts of (n-1)-gram contexts (Counter object).\n","    - n: The n-gram size (2 for bigrams, 3 for trigrams).\n","\n","    Output:\n","    - Probability of the sentence (float).\n","    \"\"\"\n","    tokens = word_tokenize(sentence.lower())\n","\n","    # Add appropriate start and end tokens based on n\n","    if n == 2:\n","        tokens = ['<s>'] + tokens + ['</s>']\n","        ngrams = list(bigrams(tokens))\n","    elif n == 3:\n","        tokens = ['<s>', '<s>'] + tokens + ['</s>']\n","        ngrams = list(trigrams(tokens))\n","\n","    sentence_probability = 1.0  # Initialize sentence probability as 1\n","\n","    # your code here\n","    for ng in ngrams:\n","        context = ng[:-1]\n","        ngram_prob = ngram_counts[ng] / context_counts[context] if ng in ngram_counts else 0.0001\n","        sentence_probability *= ngram_prob\n","\n","    return sentence_probability\n","\n","\n","def calculate_log_sentence_probability(sentence, ngram_counts, context_counts, n):\n","    \"\"\"\n","    Calculate the log probability of a sentence using an n-gram model without smoothing.\n","\n","    Input:\n","    - sentence: A sentence (string).\n","    - ngram_counts: Counts of n-grams (Counter object).\n","    - context_counts: Counts of (n-1)-gram contexts (Counter object).\n","    - n: The n-gram size (2 for bigrams, 3 for trigrams).\n","\n","    Output:\n","    - Log probability of the sentence (float).\n","    \"\"\"\n","    tokens = word_tokenize(sentence.lower())\n","\n","    # Add appropriate start and end tokens based on n\n","    if n == 2:\n","        tokens = ['<s>'] + tokens + ['</s>']\n","        ngrams = list(bigrams(tokens))\n","    elif n == 3:\n","        tokens = ['<s>', '<s>'] + tokens + ['</s>']\n","        ngrams = list(trigrams(tokens))\n","\n","    log_sentence_probability = 0.0  # Initialize log probability as 0\n","\n","    #your code here\n","    for ng in ngrams:\n","        context = ng[:-1]\n","        ngram_prob = ngram_counts[ng] / context_counts[context] if ng in ngram_counts else 0.0001\n","        log_sentence_probability += math.log(ngram_prob)\n","        \n","    return log_sentence_probability\n","\n","\n","# Example usage\n","bigram_counts, bigram_contexts = calculate_ngram_probabilities(corpus, 2)\n","trigram_counts, trigram_contexts = calculate_ngram_probabilities(corpus, 3)\n","\n","# Example sentences\n","test_sentences = [\n","    \"I enjoy chocolate cake\",\n","    \"I hate chocolate cake\",\n","    \"He told me about it\"\n","]\n","\n","for sent in test_sentences:\n","    print(\"\\t\\t---------------------------\\t\\t\")\n","    # Calculate normal probabilities using bigram and trigram models\n","    bigram_sentence_prob = calculate_sentence_probability(sent, bigram_counts, bigram_contexts, 2)\n","    print(f\"Bigram model probability of '{sent}': {bigram_sentence_prob}\")\n","print(\"----------------------------------------------------------------\")\n","for sent in test_sentences:\n","    print(\"\\t\\t---------------------------\\t\\t\")\n","    # Log probabilities using bigram and trigram models\n","    log_bigram_sentence_prob = calculate_log_sentence_probability(sent, bigram_counts, bigram_contexts, 2)\n","    # Output for bigram model\n","    print(f\"Log bigram model probability of '{sent}': {log_bigram_sentence_prob}\")\n","print(\"----------------------------------------------------------------\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MbmxgegA1s77"},"source":["# Exercise 2: Smoothing Techniques\n","\n","In this exercise, we will explore various smoothing techniques used in language models to enhance the accuracy of probability estimates. Smoothing is essential in natural language processing, particularly when dealing with sparse data, where certain n-grams may not appear in the training set, leading to zero probabilities. By applying smoothing methods, we can adjust these probabilities to account for unseen data.\n","\n","## Purpose of Smoothing in Language Models\n","\n","Smoothing techniques aim to prevent zero probabilities for n-grams that do not occur in the training corpus. This is particularly important for language models, as a zero probability would imply that a particular sequence of words is impossible, which is not realistic in natural language. Smoothing helps create a more robust model that can generalize better to new, unseen text.\n","\n","## Smoothing Methods\n","\n","### 1. Add-One Smoothing\n","Also known as Laplace smoothing, this method adds one to the count of each n-gram and the vocabulary size to the denominator. The formula for calculating the probability in the case of trigrams is:\n","\n","$$\n","P(w_i | w_{i-2}, w_{i-1}) = \\frac{C(w_{i-2}, w_{i-1}, w_i) + 1}{C(w_{i-2}, w_{i-1}) + V}\n","$$\n","\n","where:\n","- $ C(w_{i-2}, w_{i-1}, w_i) $ is the count of the trigram $ (w_{i-2}, w_{i-1}, w_i) $.\n","- $ C(w_{i-2}, w_{i-1}) $ is the count of the context bigram $ (w_{i-2}, w_{i-1}) $.\n","- $ V $ is the vocabulary size.\n","\n","### 2. Add-k Smoothing\n","A generalization of add-one smoothing, this method adds a constant \\( k \\) (where \\( k > 0 \\)) to the counts of all n-grams. The probability in the case of trigrams is computed as follows:\n","\n","$$\n","P(w_i | w_{i-2}, w_{i-1}) = \\frac{C(w_{i-2}, w_{i-1}, w_i) + k}{C(w_{i-2}, w_{i-1}) + k \\cdot V}\n","$$\n","\n","where \\( k \\) is a small constant (e.g., \\( k=0.5 \\)).\n","\n","### 3. Interpolation\n","This method combines probabilities from different n-gram models (e.g., unigrams, bigrams, trigrams) using weighted averages. The interpolated probability in case of trigrams can be expressed as:\n","\n","$$\n","P_{\\text{interp}}(w_i | w_{i-1}) = \\lambda_1 P_{1}(w_i) + \\lambda_2 P_{2}(w_{i-1}, w_i) + \\lambda_3 P_{3}(w_{i-2}, w_{i-1}, w_i)\n","$$\n","\n","where $ \\lambda_1, \\lambda_2, \\lambda_3 $ are weights that sum to 1, and $ P_{1}, P_{2}, P_{3} $ are the probabilities from the unigram, bigram, and trigram models, respectively.\n","\n","### 4. Stupid Backoff\n","Stupid backoff uses a simple heuristic to handle zero probabilities. If a trigram has a zero probability, it backs off to the lower-order n-grams with discounted probabilities. The probability for trigrams can be expressed as:\n","\n","$$\n","P_{\\text{backoff}}(w_i | w_{i-2}, w_{i-1}) =\n","\\begin{cases}\n","\\frac{C(w_{i-2}, w_{i-1}, w_i)}{C(w_{i-2}, w_{i-1})} & \\text{if } C(w_{i-2}, w_{i-1}, w_i) > 0 \\\\\n","\\alpha \\cdot P_{\\text{backoff}}(w_i | w_{i-1}) & \\text{otherwise}\n","\\end{cases}\n","$$\n","\n","If the bigram probability $ P_{\\text{backoff}}(w_i | w_{i-1}) $ is also zero, the model will further back off to the unigram probability:\n","\n","$$\n","P_{\\text{backoff}}(w_i | w_{i-1}) =\n","\\begin{cases}\n","\\frac{C(w_{i-1}, w_i)}{C(w_{i-1})} & \\text{if } C(w_{i-1}, w_i) > 0 \\\\\n","\\alpha \\cdot P_{\\text{backoff}}(w_i) & \\text{otherwise}\n","\\end{cases}\n","$$\n","\n","where $ \\alpha $ is a discount factor (typically set to $ 0.4 $ or similar), and $ P_{\\text{backoff}}(w_i) $ is the probability of the unigram.\n","\n","\n","## Questions:\n","1. Implement the four aforementioned smoothing methods for both bigram and trigram models.\n","\n","  **Note:** Returns log probability to avoid numerical underflow with long sequences.\n","\n","2. Using the smoothing methods described above, apply each technique to recalculate the probabilities of the sentences from Exercise 1:\n","\n","  *   \"I enjoy chocolate cake\"\n","  *   \"I hate chocolate cake\"\n","  *    \"He told me about it\"\n","\n","3. Which smoothing method seems to work best for the test sentences used in exercise 1, and why? Discuss the strengths and weaknesses of each method in the context of the sentences analyzed.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKwuVZj21s77","outputId":"3cbb98eb-a7ab-45fd-f514-03535d573fee"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Analyzing sentence: 'I enjoy chocolate cake'\n","\n","Bigram Models (n=2):\n","Add-one smoothing: Log probability = 0.0\n","\n","Trigram Models (n=3):\n","Add-one smoothing: Log probability = 0.0\n","\n","Analyzing sentence: 'I hate chocolate cake'\n","\n","Bigram Models (n=2):\n","Add-one smoothing: Log probability = 0.0\n","\n","Trigram Models (n=3):\n","Add-one smoothing: Log probability = 0.0\n","\n","Analyzing sentence: 'He told me about it'\n","\n","Bigram Models (n=2):\n","Add-one smoothing: Log probability = 0.0\n","\n","Trigram Models (n=3):\n","Add-one smoothing: Log probability = 0.0\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","from nltk import bigrams, trigrams\n","from collections import Counter\n","from nltk.tokenize import word_tokenize\n","import math\n","\n","def get_vocabulary_size(sentences):\n","    \"\"\"\n","    Helper function to compute the vocabulary size from a list of sentences.\n","\n","    Parameters:\n","    - sentences (list): List of strings, each string being a sentence\n","\n","    Returns:\n","    - int: Number of unique words in the vocabulary\n","    \"\"\"\n","    vocab = set()\n","\n","    # your code here\n","\n","    return len(vocab)\n","\n","\n","def add_one_smoothing(sentence, ngram_counts, context_counts, vocab_size, n):\n","    \"\"\"\n","    Implements Add-One (Laplace) smoothing for n-gram language models.\n","\n","    Mathematical equation:\n","    P(w_i|w_{i-n+1}^{i-1}) = [C(w_{i-n+1}^i) + 1] / [C(w_{i-n+1}^{i-1}) + |V|]\n","\n","    Parameters:\n","    - sentence (str): Input sentence to calculate probability for\n","    - ngram_counts (Counter): Dictionary of n-gram counts from training data\n","    - context_counts (Counter): Dictionary of (n-1)-gram counts from training data\n","    - vocab_size (int): Size of vocabulary in training data\n","    - n (int): Order of n-gram model (2 for bigram, 3 for trigram)\n","\n","    Returns:\n","    - float: Log probability of the sentence under the smoothed model\n","    \"\"\"\n","    tokens = word_tokenize(sentence.lower())\n","    tokens = ['<s>'] * (n - 1) + tokens + ['</s>']\n","\n","\n","    sentence_probability = 0.0\n","\n","    # your code here\n","\n","    return sentence_probability\n","\n","def add_k_smoothing(sentence, ngram_counts, context_counts, vocab_size, n, k=0.5):\n","    \"\"\"\n","    Implements Add-k smoothing (also known as Lidstone smoothing) for n-gram language models.\n","\n","    Mathematical equation:\n","    P(w_i|w_{i-n+1}^{i-1}) = [C(w_{i-n+1}^i) + k] / [C(w_{i-n+1}^{i-1}) + k|V|]\n","    where:\n","    - k is the smoothing parameter (typically 0 < k < 1)\n","    - C(w_{i-n+1}^i) is the count of the n-gram\n","    - C(w_{i-n+1}^{i-1}) is the count of the context\n","    - |V| is the vocabulary size\n","\n","    Parameters:\n","    - sentence (str): Input sentence to calculate probability for\n","    - ngram_counts (Counter): Dictionary of n-gram counts from training data\n","    - context_counts (Counter): Dictionary of (n-1)-gram counts from training data\n","    - vocab_size (int): Size of vocabulary in training data\n","    - n (int): Order of n-gram model (2 for bigram, 3 for trigram)\n","    - k (float): Smoothing parameter, defaults to 0.5\n","\n","    Returns:\n","    - float: Log probability of the sentence under the smoothed model\n","    \"\"\"\n","    tokens = word_tokenize(sentence.lower())\n","\n","    if n == 2:\n","        tokens = ['<s>'] + tokens + ['</s>']\n","        ngrams = list(bigrams(tokens))\n","    elif n == 3:\n","        tokens = ['<s>', '<s>'] + tokens + ['</s>']\n","        ngrams = list(trigrams(tokens))\n","\n","    sentence_probability = 0.0\n","\n","    # your code here\n","\n","    return sentence_probability\n","\n","def interpolation_smoothing(sentence, unigram_counts, ngram_counts, context_counts,\n","                          total_words, n, lambdas=None):\n","    \"\"\"\n","    Implements linear interpolation smoothing for n-gram models (n=2 or n=3).\n","\n","    Mathematical equation for bigrams (n=2):\n","    P(w_i|w_{i-1}) = λ₁P(w_i) + λ₂P(w_i|w_{i-1})\n","\n","    For trigrams (n=3):\n","    P(w_i|w_{i-2}w_{i-1}) = λ₁P(w_i) + λ₂P(w_i|w_{i-1}) + λ₃P(w_i|w_{i-2}w_{i-1})\n","\n","    # We handle the zero probability case (interpolated proba) by using a very small value of 0.0001\n","\n","    Parameters:\n","    - sentence (str): Input sentence\n","    - unigram_counts (Counter): Dictionary of unigram counts\n","    - ngram_counts (Counter): Dictionary of n-gram counts\n","    - context_counts (dict): Dictionary of context counts for different n-gram orders\n","    - total_words (int): Total word count in training data\n","    - n (int): Order of n-gram model (2 or 3)\n","    - lambdas (list): Interpolation weights. For bigrams: [λ₁, λ₂], for trigrams: [λ₁, λ₂, λ₃]\n","\n","    Returns:\n","    - float: Log probability of the sentence under the interpolated model\n","    \"\"\"\n","    tokens = word_tokenize(sentence.lower())\n","\n","    # Set default lambdas if not provided\n","    if lambdas is None:\n","        if n == 2:\n","            lambdas = [0.3, 0.7]  # [unigram, bigram]\n","        else:\n","            lambdas = [0.1, 0.3, 0.6]  # [unigram, bigram, trigram]\n","\n","    # Add appropriate start/end tokens\n","    if n == 2:\n","        tokens = ['<s>'] + tokens + ['</s>']\n","        start_index = 1\n","    else:  # n == 3\n","        tokens = ['<s>', '<s>'] + tokens + ['</s>']\n","        start_index = 2\n","\n","    sentence_probability = 0.0\n","\n","    # your code here\n","\n","    return sentence_probability\n","\n","def stupid_backoff(sentence, unigram_counts, ngram_counts, total_words,\n","                  context_counts, n, alpha=0.4):\n","    \"\"\"\n","    Implements Stupid Backoff smoothing for n-gram models (n=2 or n=3).\n","\n","    Mathematical equation for bigrams (n=2):\n","    S(w_i|w_{i-1}) =\n","        count(w_{i-1},w_i) / count(w_{i-1})     if count(w_{i-1},w_i) > 0\n","        α * count(w_i) / N                       otherwise\n","\n","    For trigrams (n=3):\n","    S(w_i|w_{i-2},w_{i-1}) =\n","        count(w_{i-2},w_{i-1},w_i) / count(w_{i-2},w_{i-1})   if trigram exists\n","        α * S(w_i|w_{i-1})                                     otherwise\n","\n","    # We handle the zero probability case (when unigram count is zero) by using a very small value of 0.0001\n","\n","    Parameters:\n","    - sentence (str): Input sentence\n","    - unigram_counts (Counter): Dictionary of unigram counts\n","    - ngram_counts (dict): Dictionary of n-gram counts for different orders\n","    - total_words (int): Total word count in training data\n","    - context_counts (dict): Dictionary of context counts for different n-gram orders\n","    - n (int): Order of n-gram model (2 or 3)\n","    - alpha (float): Backoff penalty parameter\n","\n","    Returns:\n","    - float: Log score of the sentence\n","    \"\"\"\n","    tokens = word_tokenize(sentence.lower())\n","\n","    if n == 2:\n","        tokens = ['<s>'] + tokens + ['</s>']\n","        start_index = 1\n","    else:  # n == 3\n","        tokens = ['<s>', '<s>'] + tokens + ['</s>']\n","        start_index = 2\n","\n","    sentence_probability = 0.0\n","\n","    # your code here\n","\n","    return sentence_probability\n","\n","\n","\n","\"\"\"\n","Main function demonstrating the usage of different smoothing methods with both\n","bigram and trigram models, as well as calculating and printing perplexities.\n","\"\"\"\n","nltk.download('punkt')\n","\n","corpus = [\n","    \"I love chocolate cake\",\n","    \"I love chocolate ice cream\",\n","    \"I love cookies\",\n","    \"I enjoy vanilla cake\",\n","    \"I enjoy vanilla ice cream\",\n","    \"I enjoy cookies\",\n","    \"Chocolate is my favorite\",\n","    \"Vanilla cookies are great\",\n","    \"Chocolate cookies are also great\"\n","]\n","\n","# Initialize counts\n","vocab_size = get_vocabulary_size(corpus)\n","unigram_counts = Counter()\n","\n","# Create separate counters for different n-gram orders\n","ngram_counts = {1: Counter(), 2: Counter(), 3: Counter()}  # 1:unigram, 2:bigram, 3:trigram\n","context_counts = {1: Counter(), 2: Counter()}  # 1:unigram context, 2:bigram context\n","\n","# Process corpus\n","for sentence in corpus:\n","    tokens = word_tokenize(sentence.lower())\n","    unigram_counts.update(tokens)\n","\n","    # Add appropriate padding for different n-grams\n","    bigram_tokens = ['<s>'] + tokens + ['</s>']\n","    trigram_tokens = ['<s>', '<s>'] + tokens + ['</s>']\n","\n","    # Update counts\n","    ngram_counts[1].update(tokens)  # unigrams\n","    for bigram in bigrams(bigram_tokens):\n","        ngram_counts[2][bigram] += 1\n","        context_counts[1][bigram[:-1]] += 1\n","    for trigram in trigrams(trigram_tokens):\n","        ngram_counts[3][trigram] += 1\n","        context_counts[2][trigram[:-1]] += 1\n","\n","total_words = sum(unigram_counts.values())\n","\n","# Test sentences\n","test_sentences = [\n","    \"I enjoy chocolate cake\",\n","    \"I hate chocolate cake\",\n","    \"He told me about it\"\n","]\n","\n","# Test each smoothing method with both bigrams and trigrams\n","for sentence in test_sentences:\n","    print(f\"\\nAnalyzing sentence: '{sentence}'\")\n","    sentence_length = len(word_tokenize(sentence))\n","\n","    print(\"\\nBigram Models (n=2):\")\n","    for method, func in [(\"Add-one smoothing\", add_one_smoothing)]:\n","        log_prob = func(sentence, ngram_counts[2], context_counts[1], vocab_size, 2)\n","        print(f\"{method}: Log probability = {log_prob}\")\n","\n","    print(\"\\nTrigram Models (n=3):\")\n","    for method, func in [(\"Add-one smoothing\", add_one_smoothing)]:\n","        log_prob = func(sentence, ngram_counts[3], context_counts[2], vocab_size, 3)\n","        print(f\"{method}: Log probability = {log_prob}\")"]},{"cell_type":"markdown","metadata":{"id":"0Z1WdRTx1s79"},"source":["# Exercise 3: Perplexity Evaluation\n","\n","In this exercise, we will evaluate the performance of your n-gram language model using perplexity. This metric is widely used to assess how well a probability distribution predicts a sample.\n","\n","## Understanding Perplexity\n","\n","**Perplexity** is defined as the exponentiation of the average negative log-likelihood of a sequence. It provides insight into how effectively a language model can predict a given sequence of words. A lower perplexity score indicates that the model is more confident in its predictions, while a higher score suggests greater uncertainty.\n","\n","The formula for calculating perplexity for a test set is:\n","\n","$$\n","\\text{Perplexity} = e^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_{i-1})}\n","$$\n","\n","where:\n","- $N$ is the total number of words in the test set.\n","- $P(w_i | w_{i-1})$ is the probability of the word $w_i$ given its preceding word $w_{i-1}$ as predicted by your n-gram model.\n","\n","### Significance of Perplexity\n","\n","Perplexity is useful for comparing different language models; a model with lower perplexity generally performs better. It effectively captures how well the model generalizes to unseen data.\n","\n","## Implementation\n","- Use the testing part to evaluate the two language models built previously using perplexity and using the diffrent smoothing methods.\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":510,"status":"ok","timestamp":1730060316503,"user":{"displayName":"Mohamed Hadj Ameur","userId":"13287992149799244353"},"user_tz":-60},"id":"tFjRerLQ1s79"},"outputs":[],"source":["def calculate_perplexity(log_probability, sentence_length):\n","    \"\"\"\n","    Calculate the perplexity of a sentence.\n","\n","    Parameters:\n","    - log_probability (float): Total log probability of the sentence\n","    - sentence_length (int): Length of the sentence in words\n","\n","    Returns:\n","    - float: Perplexity of the sentence\n","    \"\"\"\n","    return 0\n","\n","# your code here"]},{"cell_type":"markdown","metadata":{"id":"Pv45fdgD1s78"},"source":["# Exercise 4: Sentence Generation\n","\n","In this exercise, we will focus on generating sentences using a trained n-gram language model. We will implement a random generation method that selects the next word from the top five most probable words based on a specific context. This will help us understand how context influences word choice in language generation.\n","\n","## Task: Generate Sentences Using N-gram Models\n","\n","You will implement a function to generate sentences based on bigram, trigram, and four-gram models. Additionally, you will utilize the Add-One smoothing method to calculate and print the probabilities of the generated sentences.\n","\n","### Text Corpus\n","\n","For this exercise, you will use the **Gutenberg corpus** from the NLTK library. You can access this corpus by importing the required module and selecting a suitable text, such as \"Alice's Adventures in Wonderland\" or any other available text.\n","\n","### Steps to Complete:\n","\n","1. **Sampling Sentences**:\n","   - Implement a function that generates sentences by:\n","     - Starting with a seed word or phrase.\n","     - Using the bigram model to select the next word based on the current context.\n","     - Repeating the process using the trigram model and then the four-gram model.\n","     - For each generated word, select from the top five most probable candidates.\n","\n","2. **Calculate Probabilities**:\n","   - Use different smoothing methods to compute and print the probability of each generated sentence.\n","   - Analyze which smoothing method yields the best results.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFAUDmjeCSIf"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"agk64cTraLT7"},"source":["# Exercise 5: Simplified Bigram EM Algorithm\n","\n","## Background\n","The EM (Expectation-Maximization) algorithm is often used in NLP to optimize mixture weights for combining different language models, such as unigram and bigram models. This exercise explores a simplified version of this approach, focusing on calculating and updating mixture weights for unigram and bigram probabilities using a small corpus.\n","\n","## Corpus with Sentence Boundaries\n","We are given a small corpus with three sentences and sentence boundaries, as follows:\n","\n","1. `a b a`\n","2. `b c b`\n","3. `a b c`\n","\n","## Steps of the EM Algorithm\n","1. **Initialization**: Start with initial values for the mixture weights: λ₁ = 0.5 and λ₂ = 0.5.\n","   \n","2. **E-Step (Expectation Step)**:\n","   - For each bigram $(w_{i-1}, w_i)$ in the corpus, calculate the combined probability (total_prob) using the current λ values:\n","\n","$$\n","\\text{total_prob}_{\\text{(w_{i-1}, w_i)}} = \\lambda_1 \\cdot P_{\\text{unigram}}(w_i) + \\lambda_2 \\cdot P_{\\text{bigram}}(w_{i-1}, w_i)\n","$$\n","     \n","   - Compute the unigram and bigram contributions for each bigram:\n","     - **Unigram contribution**: $ \\frac{\\lambda_1 \\cdot P_{\\text{unigram}}(w_i)}{\\text{total_prob}} $\n","     - **Bigram contribution**: $ \\frac{\\lambda_2 \\cdot P_{\\text{bigram}}(w_{i-1}, w_i)}{\\text{total_prob}} $\n","\n","3. **M-Step (Maximization Step)**:\n","   - Sum up the unigram and bigram contributions across all bigrams in the corpus.\n","   - Update the λ values using the total unigram and bigram contributions:\n","     $$\n","     \\lambda_1 = \\frac{\\text{Total Unigram Contributions}}{\\text{Total Contributions (Unigram + Bigram)}}\n","     $$\n","     $$\n","     \\lambda_2 = \\frac{\\text{Total Bigram Contributions}}{\\text{Total Contributions (Unigram + Bigram)}}\n","     $$\n","\n","4. **Iteration and Convergence**:\n","   - Repeat the E-step and M-step until the changes in λ values are below a specified threshold (e.g., 0.001).\n","\n","## Questions\n","\n","1. What are the updated values of λ₁ and λ₂ after the first EM step?\n","2. What are the final values of λ₁ and λ₂ after the algorithm has converged?\n","3. How many iterations did it take for the algorithm to converge?\n","4. How would the results change if λ₁ and λ₂ were initialized with different values?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLm_DSCWYlMw"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-27K8MASnGQe"},"source":["# Exercise 6: Training and Evaluating an n-gram Language Model using KenLM\n","\n","## Objective\n","Learn to train an n-gram language model using KenLM, apply different smoothing techniques, and evaluate the model’s performance through perplexity. You’ll also print the model vocabulary, calculate sentence probabilities, and generate random sentences.\n","\n","## Instructions\n","\n","### 1. Setup\n","- Load the provided corpus, **\"TED2013.ar-en.en,\"** and use it to train a KenLM trigram language model.\n","\n","### 2. Sentence Probability Calculation\n","- Use the trained model to calculate the probability of a given sentence.\n","\n","### 3. Sentence Smoothness Evaluation\n","- Use the model to rank sentences based on their linguistic smoothness.\n","\n","### 4. Arabic Usage\n","- Perform the same tasks using the provided Arabic corpus **\"TED2013.ar-en.ar,\"**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPS-5cTUnWC1"},"outputs":[],"source":["# Step 1: Install KenLM if not already installed\n","# If KenLM is not installed, use the following command in the terminal to install it:\n","!pip install kenlm\n","!git clone https://github.com/kpu/kenlm.git\n","!cd kenlm && mkdir build && cd build && cmake .. && make -j4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5zw6kXSTnZPN"},"outputs":[],"source":["!kenlm/build/bin/lmplz -h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUsAZ9xXnaWM"},"outputs":[],"source":["import kenlm\n","import random\n","import os\n","from collections import defaultdict\n","\n","def train_ngram_model(corpus_path, model_path, order=3):\n","    \"\"\"\n","    Goal:\n","        Train an n-gram model using KenLM on a specified corpus and save the model.\n","\n","    Input:\n","        - corpus_path (str): Path to the corpus text file for training the model.\n","        - model_path (str): Path to save the trained ARPA model file.\n","        - order (int): Order of the n-gram model (e.g., 3 for a trigram model).\n","\n","    Output:\n","        - Saves the trained model as an ARPA file at the specified model_path.\n","    \"\"\"\n","    from subprocess import run\n","\n","    command = [\n","        'kenlm/build/bin/lmplz',  # Full path to lmplz in Colab\n","        '--order', str(order),\n","        '--text', corpus_path,\n","        '--arpa', model_path,\n","        '--temp_prefix', '/tmp'\n","    ]\n","\n","    print(f\"Training a {order}-gram model...\")\n","    result = run(command, capture_output=True, text=True)\n","\n","    # Print stdout and stderr to diagnose issues\n","    print(result.stdout)\n","    if result.stderr:\n","        print(\"Error output:\", result.stderr)\n","\n","    print(f\"Model saved to {model_path}\")\n","\n","def load_ngram_probabilities(model_path):\n","    \"\"\"\n","    Goal:\n","        Load n-gram probabilities from an ARPA file into a nested dictionary structure.\n","\n","    Input:\n","        - model_path (str): Path to the ARPA model file to load.\n","\n","    Output:\n","        - dict: A nested dictionary where each key is a context (tuple of words)\n","                and values are dictionaries mapping next words to their probabilities.\n","    \"\"\"\n","    ngram_probs = defaultdict(lambda: defaultdict(float))\n","    current_order = 0\n","\n","    with open(model_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('\\\\'):\n","                if 'grams:' in line:\n","                    current_order = int(line[1])\n","                continue\n","\n","            if not line or line.startswith('\\\\'):\n","                continue\n","\n","            parts = line.split('\\t')\n","            if len(parts) >= 2:\n","                prob = float(parts[0])\n","                ngram = tuple(parts[1].split())\n","                if len(ngram) == current_order:\n","                    context = ngram[:-1]\n","                    next_word = ngram[-1]\n","                    ngram_probs[context][next_word] = prob\n","\n","    return ngram_probs\n","\n","def calculate_sentence_probability(model, sentence):\n","    \"\"\"\n","    Calculate the probability of a given sentence using the KenLM model.\n","\n","    Parameters:\n","        model (kenlm.Model): The trained KenLM model.\n","        sentence (str): The sentence whose probability needs to be calculated.\n","\n","    Returns:\n","        float: The probability of the sentence.\n","    \"\"\"\n","    # Calculate log probability\n","    log_prob = model.score(sentence, bos=True, eos=True)\n","    probability = 10 ** (log_prob / 10)  # Assuming KenLM logs are base 10\n","    return probability\n","\n","def sort_sentences_by_smoothness(model, test_sentences):\n","   # add you code here\n","   pass\n","\n","\n","# Example usage\n","def main():\n","    model_path = 'ngram_model.arpa'\n","\n","    # Load the KenLM model\n","    model = kenlm.Model(model_path)\n","\n","    # Define test sentences\n","    test_sentences1 = [\n","        \"The ocean is vast and mysterious.\",\n","        \"ocean The mysterious is vast.\",\n","        \"The vast ocean is mysterious.\",\n","        \"The mysterious vast ocean is and.\",\n","    ]\n","\n","    test_sentences2 = [\n","        \"The sun rises in the east and sets in the west.\",\n","        \"The rises sun in the east and sets in the west.\",\n","        \"The sun rises in the west and sets in the east.\",\n","        \"The sun west rises in east the  and sets in the.\"\n","    ]\n","\n","    # your code here\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{"id":"07zgqD4zFt5d"},"source":["# **Appendix: Detailed Calculations for the First EM Step (Exercise 5)**\n","\n","## Initial Setup\n","- Initial λ₁ = 0.5, λ₂ = 0.5\n","\n","## Corpus with Sentence Boundaries\n","Sentences = [\n","* `['<s>', 'a', 'b', 'a', '</s>']`,\n","* `['<s>', 'b', 'c', 'b', '</s>']`,\n","* `['<s>', 'a', 'b', 'c', '</s>']`\n","]\n","\n","## Unigram Counts\n","- Count of \"\\<s>\": 3\n","- Count of \"a\": 3\n","- Count of \"b\": 4\n","- Count of \"c\": 2\n","- Count of \"\\</s>\": 3\n","- **Total** = 15\n","\n","## Unigram Probabilities\n","$$\n","P_{\\text{unigram}} =\n","\\begin{cases}\n","    P(\\text{\"<s>\"}) = \\frac{3}{15} = 0.2000 \\\\\n","    P(\\text{\"a\"}) = \\frac{3}{15} = 0.2000 \\\\\n","    P(\\text{\"b\"}) = \\frac{4}{15} \\approx 0.2667 \\\\\n","    P(\\text{\"c\"}) = \\frac{2}{15} \\approx 0.1333 \\\\\n","    P(\\text{\"</s>\"}) = \\frac{3}{15} = 0.2000\n","\\end{cases}\n","$$\n","\n","## Bigram Counts\n","- Count of (\"\\<s>\", \"a\"): 2\n","- Count of (\"\\<s>\", \"b\"): 1\n","- Count of (\"a\", \"b\"): 2\n","- Count of (\"b\", \"a\"): 1\n","- Count of (\"b\", \"c\"): 2\n","- Count of (\"c\", \"b\"): 1\n","- Count of (\"a\", \"\\</s>\"): 1\n","- Count of (\"b\", \"\\</s>\"): 1\n","- Count of (\"c\", \"\\</s>\"): 1\n","\n","## Bigram Probabilities\n","$$\n","P_{\\text{bigram}} =\n","\\begin{cases}\n","    P(\\text{\"<s>\", \"a\"}) = \\frac{2}{3} \\approx 0.6667 \\\\\n","    P(\\text{\"<s>\", \"b\"}) = \\frac{1}{3} \\approx 0.3333 \\\\\n","    P(\\text{\"a\", \"b\"}) = \\frac{2}{3} \\approx 0.6667 \\\\\n","    P(\\text{\"b\", \"a\"}) = \\frac{1}{4} = 0.25 \\\\\n","    P(\\text{\"b\", \"c\"}) = \\frac{2}{4} = 0.5 \\\\\n","    P(\\text{\"c\", \"b\"}) = \\frac{1}{2} = 0.5 \\\\\n","    P(\\text{\"a\", \"</s>\"}) = \\frac{1}{3} \\approx 0.3333 \\\\\n","    P(\\text{\"b\", \"</s>\"}) = \\frac{1}{4} = 0.25 \\\\\n","    P(\\text{\"c\", \"</s>\"}) = \\frac{1}{2} = 0.5\n","\\end{cases}\n","$$\n","\n","# Step 1: E-Step Calculation for Each Sentence\n","\n","We will be using the below formula for calculating $\\text{total\\_prob}$ for the bigram $(w_{i-1}, w_i)$ which combines the unigram and bigram probabilities using the initial values of $\\lambda_1$ and $\\lambda_2$:\n","\n","$$\n","\\text{total_prob}_{\\text{(w_{i-1}, w_i)}} = \\lambda_1 \\cdot P_{\\text{unigram}}(w_i) + \\lambda_2 \\cdot P_{\\text{bigram}}(w_{i-1}, w_i)\n","$$\n","### First Sentence: [\"\\<s>\", \"a\", \"b\", \"a\", \"\\</s>\"]\n","\n","1. **Bigram (\"\\<s>\", \"a\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2000 + 0.5 \\times 0.6667 = 0.4334 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2000)}{0.4334} = 0.2307 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.6667)}{0.4334} = 0.7693 $\n","\n","2. **Bigram (\"a\", \"b\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2667 + 0.5 \\times 0.6667 = 0.4667 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2667)}{0.4667} = 0.2857 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.6667)}{0.4667} = 0.7143 $\n","\n","3. **Bigram (\"b\", \"a\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2000 + 0.5 \\times 0.2500 = 0.2250 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2000)}{0.2250} = 0.4444 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.2500)}{0.2250} = 0.5556 $\n","\n","4. **Bigram (\"a\", \"\\</s>\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2000 + 0.5 \\times 0.3333 = 0.2667 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2000)}{0.2667} = 0.3750 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.3333)}{0.2667} = 0.6250 $\n","\n","**Total for first sentence:**\n","- **Unigram contributions**: $0.2307 + 0.2857 + 0.4444 + 0.3750 = 1.3358$\n","- **Bigram contributions**: $0.7693 + 0.7143 + 0.5556 + 0.6250 = 2.6642$\n","\n","### Second Sentence: [\"\\<s>\", \"b\", \"c\", \"b\", \"\\</s>\"]\n","\n","1. **Bigram (\"\\<s>\", \"b\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2667 + 0.5 \\times 0.3333 = 0.3000 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2667)}{0.3000} = 0.4445 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.3333)}{0.3000} = 0.5555 $\n","\n","2. **Bigram (\"b\", \"c\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.1333 + 0.5 \\times 0.5000 = 0.3167 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.1333)}{0.3167} = 0.2105 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.5000)}{0.3167} = 0.7895 $\n","\n","3. **Bigram (\"c\", \"b\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2667 + 0.5 \\times 0.5000 = 0.3834 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2667)}{0.3834} = 0.3478 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.5000)}{0.3834} = 0.6522 $\n","\n","4. **Bigram (\"b\", \"\\</s>\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2000 + 0.5 \\times 0.2500 = 0.2250 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2000)}{0.2250} = 0.4444 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.2500)}{0.2250} = 0.5556 $\n","\n","**Total for second sentence:**\n","- **Unigram contributions**: $0.4445 + 0.2105 + 0.3478 + 0.4444 = 1.4472$\n","- **Bigram contributions**: $0.5555 + 0.7895 + 0.6522 + 0.5556 = 2.5528$\n","\n","### Third Sentence: [\"\\<s>\", \"a\", \"b\", \"c\", \"\\</s>\"]\n","\n","1. **Bigram (\"\\<s>\", \"a\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2000 + 0.5 \\times 0.6667 = 0.4334 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2000)}{0.4334} = 0.2307 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.6667)}{0.4334} = 0.7693 $\n","\n","2. **Bigram (\"a\", \"b\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2667 + 0.5 \\times 0.6667 = 0.4667 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2667)}{0.4667} = 0.2857 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.6667)}{0.4667} = 0.7143 $\n","\n","3. **Bigram (\"b\", \"c\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.1333 + 0.5 \\times 0.5000 = 0.3167 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.1333)}{0.3167} = 0.2105 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.5000)}{0.3167} = 0.7895 $\n","\n","4. **Bigram (\"c\", \"\\</s>\")**\n","   - $ \\text{total_prob} = 0.5 \\times 0.2000 + 0.5 \\times 0.5000 = 0.3500 $\n","   - $ \\text{unigram_contribution} = \\frac{(0.5 \\times 0.2000)}{0.3500} = 0.2857 $\n","   - $ \\text{bigram_contribution} = \\frac{(0.5 \\times 0.5000)}{0.3500} = 0.7143 $\n","\n","**Total for third sentence:**\n","- **Unigram contributions**: $0.2307 + 0.2857 + 0.2105 + 0.2857 = 1.0126$\n","- **Bigram contributions**: $0.7693 + 0.7143 + 0.7895 + 0.7143 = 2.9874$\n","\n","# Step 2: M-Step - Calculate Updated Lambda Values\n","Using the total expected counts to update λ values:\n","- **Total unigram contributions**: $1.3358 + 1.4472 + 1.0126 = 3.7956$\n","- **Total bigram contributions**: $2.6642 + 2.5528 + 2.9874 = 8.2044$\n","\n","Updated λ values:\n","$$\n","λ₁ = \\frac{3.7956}{12.0} = 0.3163\n","$$\n","$$\n","λ₂ = \\frac{8.2044}{12.0} = 0.6837\n","$$\n","\n","# Final Lambda Values\n","- $λ₁ = 0.3163$\n","- $λ₂ = 0.6837$\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env_ds","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
