{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z44tFn3NKTuK"
      },
      "source": [
        "\n",
        "# Introduction to Natural Language Processing with Python and NLTK\n",
        "\n",
        "In this notebook, we will explore the basic capabilities of Python's **Natural Language Toolkit (NLTK)** package for text analysis and processing. This lab serves as both a refresher of Python programming concepts and an introduction to fundamental NLP operations.\n",
        "\n",
        "Natural Language Processing (NLP) is a field that focuses on the interaction between computers and human language. It combines:\n",
        "- Computer Science\n",
        "- Artificial Intelligence\n",
        "- Linguistics\n",
        "\n",
        "Contents:\n",
        "- Setting up NLTK\n",
        "- Basic Text Processing\n",
        "- Word Frequency Analysis\n",
        "- Simple Text Statistics\n",
        "- Working with the Gutenberg Corpus\n",
        "- Practice Exercises\n",
        "\n",
        "What you will learn:\n",
        "- How to install and use NLTK\n",
        "- Basic text processing techniques\n",
        "- Working with text data in Python\n",
        "- Basic text analysis and statistics\n",
        "\n",
        "Prerequisites:\n",
        "- Basic Python programming knowledge\n",
        "- Familiarity with Python data structures (lists, dictionaries)\n",
        "- Understanding of basic file operations in Python\n",
        "\n",
        "Source:\n",
        "- [NLTK Book](http://www.nltk.org/book/)\n",
        "- [NLTK Documentation](https://www.nltk.org/)\n",
        "- [Python Documentation](https://docs.python.org/3/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJaJfzuMto0W"
      },
      "source": [
        "## Setting up NLTK\n",
        "\n",
        "First, we need to install NLTK. If you're running this notebook in Colab, run the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY6hV0ozto0X",
        "outputId": "f5a46d16-1ff4-47fb-c5fb-754a1963d5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/wafaa/anaconda3/envs/env_ds/lib/python3.9/site-packages (3.9.1)\n",
            "Requirement already satisfied: click in /home/wafaa/anaconda3/envs/env_ds/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/wafaa/anaconda3/envs/env_ds/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/wafaa/anaconda3/envs/env_ds/lib/python3.9/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /home/wafaa/anaconda3/envs/env_ds/lib/python3.9/site-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP881XOs8NKZ"
      },
      "source": [
        "## Downloading NLTK Packages for This Lab\n",
        "\n",
        "In this lab, we will utilize various Natural Language Toolkit (NLTK) resources to perform text processing and analysis. To ensure that we have all the necessary components, we need to download the following NLTK packages:\n",
        "\n",
        "1. **punkt**: This package is used for tokenization, which allows us to break down text into individual words or sentences.\n",
        "2. **averaged_perceptron_tagger**: This package provides a part-of-speech (POS) tagger, which helps us identify the grammatical roles of words in a sentence.\n",
        "3. **gutenberg**: This package includes a collection of literary texts from Project Gutenberg, which we will use for our text analysis tasks.\n",
        "4. **stopwords**: This package contains a list of common words (such as 'and', 'the', 'in') that are often filtered out in text processing because they do not add significant meaning.\n",
        "\n",
        "To facilitate sentence tokenization specifically, the **punkt** package is crucial, as it includes pre-trained models for segmenting text into sentences.\n",
        "\n",
        "To download these packages, you can execute the following commands in a code cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykFpKLy78MTj",
        "outputId": "7715ce78-892b-43b0-9c60-79fe672fd62c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/wafaa/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/wafaa/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package gutenberg to /home/wafaa/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/wafaa/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the necessary NLTK packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRkNaGFqto0Y"
      },
      "source": [
        "## Basic Text Processing\n",
        "\n",
        "Before starting the lab exercises, it's important to familiarize yourself with some key concepts in Natural Language Processing (NLP) using the Natural Language Toolkit (NLTK). To help solidify your understanding, please answer the following questions.\n",
        "\n",
        "---\n",
        "\n",
        "### NLTK Basics\n",
        "Consider the following sentence:\n",
        "\n",
        "\"Amir and Ahmed visited Tamanrasset last summer; they had so much fun. It was a memorable trip filled with adventure.\"\n",
        "\n",
        "#### Word Tokenization\n",
        "\n",
        "Word tokenization allows us to break down text into individual words, or tokens. This process is essential for text analysis, as it enables us to manipulate and analyze each component of the text separately.\n",
        "\n",
        "- **Question**: How do we tokenize a text into words using NLTK?\n",
        "\n",
        "  - **Hint**: Look into the `word_tokenize()` function from the `nltk.tokenize` module. This function is specifically designed to split a string into individual words based on whitespace and punctuation.\n",
        "\n",
        "  - **Task**: Tokenize the provided sentence.  \n",
        "\n",
        "#### Sentence Tokenization\n",
        "\n",
        "Sentence tokenization, or sentence segmentation, is the process of dividing a text into its constituent sentences. This is crucial for understanding the structure of a text, as it allows us to analyze individual thoughts or ideas conveyed by each sentence.\n",
        "\n",
        "- **Question**: How do we split a text into sentences using NLTK?\n",
        "\n",
        "  - **Hint**: You can use the `sent_tokenize()` function from the `nltk.tokenize` module, which identifies sentence boundaries and separates the text accordingly.\n",
        "\n",
        "  - **Task**: Apply sentence tokenization to the following text:  \n",
        "    \"Amir and Ahmed visited Tamanrasset last summer; they had so much fun. It was a memorable trip filled with adventure.\"\n",
        "\n",
        "#### Part-of-Speech (POS) Tagging\n",
        "\n",
        "Part-of-Speech (POS) tagging involves assigning grammatical categories to words in a sentence, such as nouns, verbs, and adjectives. This is essential for understanding the structure and meaning of sentences.\n",
        "\n",
        "- **Question**: What are Part-of-Speech tags, and how can we tag words in a sentence?\n",
        "\n",
        "  - **Hint**: NLTK has a function called `pos_tag()` that tags words with their respective POS.\n",
        "\n",
        "  - **Task**: Identify the nouns, verbs, and adjectives using POS tagging in the provided sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n73gm_j6Bn1z",
        "outputId": "edd7a56a-da8a-4e44-9cca-5fa2fe76ea72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Basic Text Processing ===\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/wafaa/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/share/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Basic Text Processing ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 1. Sentence Tokenization\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. Sentence Tokenization:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences)\n",
            "File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
            "File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
            "File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/wafaa/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/share/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Your text for analysis\n",
        "text = \"Amir and Ahmed visited Tamanrasset last summer; they had so much fun. It was a memorable trip filled with adventure.\"\n",
        "\n",
        "print(\"\\n=== Basic Text Processing ===\")\n",
        "\n",
        "# 1. Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"\\n1. Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "# 2. Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"\\n1. Word Tokenization:\")\n",
        "print(words)\n",
        "\n",
        "# 3. POS Tagging\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on function word_tokenize in module nltk.tokenize:\n",
            "\n",
            "word_tokenize(text, language='english', preserve_line=False)\n",
            "    Return a tokenized copy of *text*,\n",
            "    using NLTK's recommended word tokenizer\n",
            "    (currently an improved :class:`.TreebankWordTokenizer`\n",
            "    along with :class:`.PunktSentenceTokenizer`\n",
            "    for the specified language).\n",
            "    \n",
            "    :param text: text to split into words\n",
            "    :type text: str\n",
            "    :param language: the model name in the Punkt corpus\n",
            "    :type language: str\n",
            "    :param preserve_line: A flag to decide whether to sentence tokenize the text or not.\n",
            "    :type preserve_line: bool\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(word_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtBQFMYxofNe"
      },
      "source": [
        "## Python and NLTK Exercises\n",
        "\n",
        "In the following exercises, we will use the story **Alice in Wonderland** from the **Gutenberg** corpus.\n",
        "\n",
        "## Exercise 1: Book Word Analysis\n",
        "\n",
        "In this exercise, we will analyze the story to extract insights about word usage. We will focus on identifying frequently occurring words, specifically filtering by word length and grammatical categories.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "1. **Identify Frequent Words**: Our first task is to write a program that analyzes the text to find the 10 most frequently occurring words that are longer than 5 characters (don't forget to remove the stop words).\n",
        "   - **Hint**: You may find NLTK's `defaultdict` and `FreqDist` useful for counting word occurrences efficiently.\n",
        "\n",
        "2. **Analyze Grammatical Categories**: Next, we will extend our analysis to focus on the grammatical categories of words. Specifically, we will identify the 10 most frequently occurring nouns, verbs, and adjectives in the text.\n",
        "   - This will help us understand the text's focus based on the types of words used.\n",
        "\n",
        "3. **Visualization**: Finally, we will visualize the top 10 most frequent nouns, verbs, and adjectives using a plot.\n",
        "   - Visualizing this data will provide a clear representation of the word distribution and help in interpreting the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pFZDpKFoaqW",
        "outputId": "9ff2b985-206e-498a-eb91-63c0ecd6cc12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total characters: 144395\n",
            "\n",
            "First 500 characters:\n",
            " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
            "\n",
            "CHAPTER I. Down the Rabbit-Hole\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\n",
            "book her sister was reading, but it had no pictures or conversations in\n",
            "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
            "conversation?'\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy an\n",
            "\n",
            "=== Exercise 1: Book Word Analysis ===\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, pos_tag\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "\n",
        "# Load the \"Alice in Wonderland\" text from NLTK's Gutenberg corpus\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Total characters:\", len(alice_text))\n",
        "print(\"\\nFirst 500 characters:\\n\", alice_text[:500])\n",
        "\n",
        "\n",
        "def analyze_book_words():\n",
        "    \"\"\"\n",
        "    Analyzes word frequency and grammatical categories in Alice in Wonderland\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Exercise 1: Book Word Analysis ===\")\n",
        "\n",
        "\n",
        "analyze_book_words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Io6Rg1to0a"
      },
      "source": [
        "### Exercise 2: Text Statistics\n",
        "\n",
        "Write a program that tracks how often the following main characters appear together within the same paragraph: Alice, Queen, King, Rabbit, Hatter, Duchess.\n",
        "\n",
        "Output: Display character pairs and the frequency of their co-occurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1r0vOZtto0a",
        "outputId": "886a4f84-e6e5-4d37-9e00-3fee66650ec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Exercise 2: Character Co-occurrence Analysis ===\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, pos_tag\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "\n",
        "# Load the \"Alice in Wonderland\" text from NLTK's Gutenberg corpus\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def analyze_character_cooccurrence():\n",
        "    \"\"\"Analyze character co-occurrence using NLTK sentence tokenization\"\"\"\n",
        "    print(\"\\n=== Exercise 2: Character Co-occurrence Analysis ===\")\n",
        "\n",
        "    main_characters = {'Alice', 'Queen', 'King', 'Rabbit', 'Hatter', 'Duchess'}\n",
        "\n",
        "\n",
        "analyze_character_cooccurrence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA0cfncKto0a"
      },
      "source": [
        "## Exercise 3: Next Word Prediction and Sentence Generation\n",
        "\n",
        "1. **Next Word Prediction**:\n",
        "   - Develop a program that predicts the **next word** in a text given a specific word.\n",
        "   \n",
        "   Hint: Track the words that most commonly appear **immediately after** the given word in the text.\n",
        "   \n",
        "2. **Randomized Word Selection**:\n",
        "   - Modify the word prediction process so that, for a given word, the next word is chosen **randomly from the top 5 most frequent** words that typically follow it in the text.\n",
        "\n",
        "3. **Generate a Sentence**:\n",
        "   - Starting with a given word, generate a sentence of 10 words by predicting the next word based on the previous word in the sentence. Randomly choose the next word from the top 5 most frequent followers.\n",
        "\n",
        "4. **Optional Question: Consecutive Word Pair Prediction (Bigrams)**:\n",
        "   - Extend the program to predict the most frequent **pair of consecutive words** (bigrams) instead of just a single word. Modify the sentence generation process to randomly choose the next **pair of words** from the top 5 most frequent consecutive word pairs that follow a given word in the text.\n",
        "\n",
        "### Example Output:\n",
        "- Given the word **\"Alice\"**, you might generate sentences like:\n",
        "  - \"Alice was beginning to feel very tired of sitting by the\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aUkybHato0a",
        "outputId": "c174d786-8551-42b3-c691-fe0f221996fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Sentence (Next Word Prediction):\n",
            "alice\n",
            "\n",
            "Generated Sentence (Bigram Prediction):\n",
            "alice\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, pos_tag\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "\n",
        "# Load the \"Alice in Wonderland\" text from NLTK's Gutenberg corpus\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text and keep all words, including stop words\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = []\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess_text(text)\n",
        "\n",
        "# Function to track the words that come immediately after a given word\n",
        "def analyze_next_word(tokens):\n",
        "    next_word_dict = defaultdict(list)\n",
        "    # Loop through tokens and collect the next words\n",
        "    return next_word_dict\n",
        "\n",
        "# Function to track consecutive word pairs (bigrams) after a given word\n",
        "def analyze_next_bigram(tokens):\n",
        "    next_bigram_dict = defaultdict(list)\n",
        "    # Loop through tokens and collect the next bigram (pair of words)\n",
        "    return next_bigram_dict\n",
        "\n",
        "# Sentence generation using predicted next words\n",
        "def generate_sentence(start_word, next_word_dict, sentence_length=15):\n",
        "    sentence = [start_word]\n",
        "    current_word = start_word\n",
        "\n",
        "    # Generate the rest of the sentence by predicting the next word\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Sentence generation using predicted next word pairs (bigrams)\n",
        "def generate_sentence_bigram(start_word, next_bigram_dict, sentence_length=15):\n",
        "    sentence = [start_word]\n",
        "    current_word = start_word\n",
        "\n",
        "    # Generate the rest of the sentence by predicting the next bigram (pair of words)\n",
        "    return ' '.join(sentence[:sentence_length])\n",
        "\n",
        "# Example usage\n",
        "start_word = 'alice'\n",
        "\n",
        "# Step 1: Analyze the next word for each word in the text\n",
        "next_word_dict = analyze_next_word(tokens)\n",
        "\n",
        "# Step 2: Analyze the next bigram (pair of consecutive words) for each word\n",
        "next_bigram_dict = analyze_next_bigram(tokens)\n",
        "\n",
        "# Step 3: Generate a sentence using next word prediction\n",
        "print(\"Generated Sentence (Next Word Prediction):\")\n",
        "print(generate_sentence(start_word, next_word_dict))\n",
        "\n",
        "# Step 4: Generate a sentence using next bigram prediction\n",
        "print(\"\\nGenerated Sentence (Bigram Prediction):\")\n",
        "print(generate_sentence_bigram(start_word, next_bigram_dict))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-nsIlfbto0a"
      },
      "source": [
        "## Exercise 4: Chapter Vocabulary Profiler\n",
        "\n",
        "In this exercise, you will create a program that evaluates the vocabulary complexity of each chapter in the story. This analysis will provide insights into the linguistic richness of the chapters by calculating various metrics.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "1. **Average Word Length**:\n",
        "   - Calculate the average length of words in each chapter. This metric helps determine the complexity of the vocabulary used.\n",
        "\n",
        "2. **Number of Unique Words**:\n",
        "   - Count the number of unique words in each chapter. This indicates the diversity of the vocabulary.\n",
        "\n",
        "3. **Frequency of Complex Words**:\n",
        "   - Identify and count the frequency of complex words, defined as words with more than 5 or 6 letters. This metric highlights the use of advanced vocabulary.\n",
        "\n",
        "4. **Merging Metrics**:\n",
        "   - Merge the three metrics—average word length, number of unique words, and frequency of complex words—into a single vocabulary score using the following formula:\n",
        "\n",
        "   Vocabulary Score = (0.4 * Average Word Length) + (0.4 * Number of Unique Words) + (0.2 * Number of Complex Words)\n",
        "\n",
        "   Apply this formula to generate a combined score for each chapter, allowing for easier comparison between them.\n",
        "\n",
        "\n",
        "   Apply this formula to generate a combined score for each chapter, allowing for easier comparison between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EM383Tito0b",
        "outputId": "95b0ff84-ea68-4a7a-f3d4-d4afec660ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Exercise 4: Chapter Vocabulary Analysis ===\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the \"Alice in Wonderland\" text from NLTK's Gutenberg corpus\n",
        "from nltk.corpus import gutenberg\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "def analyze_chapter_vocabulary():\n",
        "    \"\"\"Analyze vocabulary complexity of each chapter and sort them by complexity\"\"\"\n",
        "    print(\"\\n=== Exercise 4: Chapter Vocabulary Analysis ===\")\n",
        "\n",
        "\n",
        "# Call the function\n",
        "analyze_chapter_vocabulary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTl6utERto0b"
      },
      "source": [
        "## Exercise 5: Character Action Tracker\n",
        "\n",
        "In this exercise, you will develop a program to identify and track the actions (verbs) most commonly associated with the main characters in the story. This analysis will help you understand the behaviors and activities of the characters throughout the narrative.\n",
        "\n",
        "### Main Characters\n",
        "\n",
        "For this analysis, we will focus on the following main characters: **Alice, Queen, King, Rabbit, Hatter, Duchess**.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "1. **Identify Actions**:\n",
        "   - Analyze the text to extract **verbs** associated with each main character. This involves tracking the occurrences of verbs that appear within a short context around the character's name.\n",
        "   - **Hint**: Focus on identifying verbs that occur **within the next 4 words** after the character's name appears in a sentence.\n",
        "\n",
        "2. **Count and Rank**:\n",
        "   - Count how many times each verb is associated with each character and rank them to find the most frequent actions.\n",
        "\n",
        "3. **Output Results**:\n",
        "   - For each main character, print a list of the top verbs associated with them, along with their frequencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0PyG2d-to0b",
        "outputId": "ee5eee31-ce99-45fb-f795-85eff3c91be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Exercise 5: Character Action Tracker ===\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist, pos_tag\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "\n",
        "# Load the \"Alice in Wonderland\" text from NLTK's Gutenberg corpus\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def track_character_actions():\n",
        "    \"\"\"Track character actions using POS tagging\"\"\"\n",
        "    print(\"\\n=== Exercise 5: Character Action Tracker ===\")\n",
        "\n",
        "\n",
        "track_character_actions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfvC9wSTto0b"
      },
      "source": [
        "## Exercise 6: Sentence Type Classification\n",
        "\n",
        "In this exercise, we will categorize sentences in the text based on their type, focusing on **Questions**, **Exclamations**, **Statements**, and **Imperatives**. By analyzing the structure of each sentence, we can gain a better understanding of how characters express themselves and how the narrative develops.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. **Questions**:\n",
        "   - Identify sentences that ask for information or clarification.\n",
        "   - Specifically, we will focus on questions that start with one of the following words:\n",
        "     - \"Who\"\n",
        "     - \"What\"\n",
        "     - \"Why\"\n",
        "     - \"Where\"\n",
        "   - Ensure that these sentences end with a question mark (`?`).\n",
        "\n",
        "2. **Exclamations**:\n",
        "   - Identify sentences that express strong emotions or surprise.\n",
        "   - These sentences typically end with an exclamation mark (`!`).\n",
        "\n",
        "3. **Statements**:\n",
        "   - Identify sentences that declare facts or provide information.\n",
        "   - These sentences usually start with a **noun** or **pronoun** (e.g., \"Alice,\" \"She,\" \"The cat\").\n",
        "   - They end with a period (`.`).\n",
        "\n",
        "4. **Imperatives**:\n",
        "   - Identify sentences that give commands, requests, or instructions.\n",
        "   - These sentences usually start with a **verb in its base form** (e.g., \"Go,\" \"Run,\" \"Take\").\n",
        "   - They can end with a period (`.`) or an exclamation mark (`!`).\n",
        "\n",
        "\n",
        "### Tasks to Complete:\n",
        "- Extract and classify sentences from the text into the four categories mentioned above.\n",
        "- Print the top 5 sentences from each category to understand the different sentence structures and their usage in the story.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xfInEaKeto0b"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "\n",
        "# Load the \"Alice in Wonderland\" text from NLTK's Gutenberg corpus\n",
        "from nltk.corpus import gutenberg\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# Function to check if a word is a noun or pronoun\n",
        "def is_noun_or_pronoun(word):\n",
        "    \"\"\"Check if a word is a noun or pronoun based on its part of speech tag.\"\"\"\n",
        "    return False\n",
        "\n",
        "\n",
        "# Function to extract and categorize sentences\n",
        "def categorize_sentences(text):\n",
        "    \"\"\"Categorize sentences into Questions, Exclamations, Statements, and Imperatives.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Initialize categories\n",
        "    categories = {'Questions': [], 'Exclamations': [], 'Statements': [], 'Imperatives': []}\n",
        "\n",
        "    return categories\n",
        "\n",
        "# Categorize the sentences in the text\n",
        "categories = categorize_sentences(alice_text)\n",
        "\n",
        "# Function to plot the frequencies of the categories\n",
        "def plot_category_frequencies(categories):\n",
        "    \"\"\"Plot the frequencies of different sentence categories.\"\"\"\n",
        "\n",
        "\n",
        "# Plot the frequencies\n",
        "plot_category_frequencies(categories)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTbTCPEEto0c"
      },
      "source": [
        "### Exercise 7: Analysis of Violent Actions and Death Mentions\n",
        "\n",
        "In this exercise, you will develop a program to identify and analyze sentences in the story that contain mentions of violent actions or death. This analysis aims to explore the themes of violence and mortality within the narrative.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. **Identify Relevant Sentences**:\n",
        "   - Write a program that scans the text for sentences containing specific keywords related to violent actions or death. Use the following keywords as indicators:\n",
        "     - **Death-Related Words**:\n",
        "       - \"kill\"\n",
        "       - \"murder\"\n",
        "       - \"die\"\n",
        "       - \"death\"\n",
        "     - **Violent Action Words**:\n",
        "       - \"attack\"\n",
        "       - \"stab\"\n",
        "       - \"shoot\"\n",
        "       - \"injure\"\n",
        "       - \"hurt\"\n",
        "       - \"blood\"\n",
        "       - \"assault\"\n",
        "\n",
        "2. **Categorize Instances**:\n",
        "   - After identifying sentences containing the relevant keywords, categorize them into two distinct groups:\n",
        "     - **Death Indicators**: Sentences containing any of the death-related words.\n",
        "     - **Violent Actions**: Sentences containing any of the violent action words.\n",
        "\n",
        "3. **Display Results**:\n",
        "   - For each category, print the top 10 sentences that correspond to each keyword. This will provide insights into how often these themes appear in the text and in what context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zOZmrRFnto0c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Load Alice in Wonderland text from the NLTK Gutenberg corpus\n",
        "alice_text = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "# Keywords related to death and violence\n",
        "death_related_words = {\"kill\", \"murder\", \"die\", \"death\"}\n",
        "violent_action_words = {\"attack\", \"stab\", \"shoot\", \"injure\", \"hurt\", \"blood\", \"assault\"}\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
