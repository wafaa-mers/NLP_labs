{"cells":[{"cell_type":"markdown","metadata":{"id":"_bQzqamqDCGZ"},"source":["# Lab 2: Regular Expressions, Edit Distance, and Subword Unit Tokenization\n","\n","In this lab, we will cover key topics, including regular expressions, edit distance, and subword unit tokenization using Byte Pair Encoding (BPE). This notebook contains exercises that students will need to complete in the code sections. The necessary files will also be provided with this notebook.\n"]},{"cell_type":"markdown","metadata":{"id":"qjRG30_FDCGf"},"source":["## Exercise 1: Basic Regular Expressions\n","\n","Write regular expressions for the following languages:\n","1. The set of all alphabetic strings\n","2. The set of all lower-case alphabetic strings ending in the letter b\n","3. The set of all strings from the alphabet a,b such that each letter a is immediately preceded by and immediately followed by the letter b"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1729444442172,"user":{"displayName":"Mohamed Hadj Ameur","userId":"13287992149799244353"},"user_tz":-60},"id":"5RQOqkXnDCGh","outputId":"64a270b1-b67f-4ef9-aacf-12d3f4a3487d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Testing regex ^[a-zA-Z]+$ which searches for all alphabetic strings:\n","\"\" matches: False\n","\"Hello\" matches: True\n","\"world\" matches: True\n","\"abc\" matches: True\n","\"bab\" matches: True\n","\"abab\" matches: True\n","\"babbabbab\" matches: True\n","\"bababab\" matches: True\n","\"abba\" matches: True\n","\n","Testing regex ^[a-z]*b$ which searches for all lower-case alphabetic strings ending in a b:\n","\"\" matches: False\n","\"Hello\" matches: False\n","\"world\" matches: False\n","\"abc\" matches: False\n","\"bab\" matches: True\n","\"abab\" matches: True\n","\"babbabbab\" matches: True\n","\"bababab\" matches: True\n","\"abba\" matches: False\n","\n","Testing regex ^(b(ab)*)*$ which searches for all strings from the alphabet a,b such that each a is immediately preceded by and immediately followed by a b:\n","\"\" matches: True\n","\"Hello\" matches: False\n","\"world\" matches: False\n","\"abc\" matches: False\n","\"bab\" matches: True\n","\"abab\" matches: False\n","\"babbabbab\" matches: True\n","\"bababab\" matches: True\n","\"abba\" matches: False\n"]}],"source":["import re\n","\n","# 1. The set of all alphabetic strings\n","regex_1 = r'^[a-zA-Z]+$'\n","\n","# 2. The set of all lower-case alphabetic strings ending in the letter b\n","regex_2 = r'^[a-z]*b$'\n","\n","# 3. The set of all strings from the alphabet a,b such that each letter a is immediately preceded by and immediately followed by the letter b\n","regex_3 = r'^(b(ab)*)*$'\n","\n","# Test the regular expressions\n","test_strings = [\"\",'Hello', 'world', 'abc', 'bab', 'abab', 'babbabbab', 'bababab', 'abba']\n","\n","for i, (regex,question) in enumerate([(regex_1,\"all alphabetic strings\"), (regex_2,\"all lower-case alphabetic strings ending in a b\"), (regex_3,\"all strings from the alphabet a,b such that each a is immediately preceded by and immediately followed by a b\")], 1):\n","    print(f\"\\nTesting regex {regex} which searches for {question}:\")\n","    for test_str in test_strings:\n","        print(f\"\\\"{test_str}\\\" matches: {bool(re.match(regex, test_str))}\")"]},{"cell_type":"markdown","metadata":{"id":"Qr3qRrrbDCGk"},"source":["## Exercise 2: Advanced Regular Expressions\n","\n","Write regular expressions for the following languages. By \"word\", we mean an alphabetic string separated from other words by whitespace, any relevant punctuation, line breaks, and so forth.\n","\n","1. The set of all strings with two consecutive repeated words (e.g., \"Humbert Humbert\" and \"the the\" but not \"the bug\" or \"the big bug\").\n","2. All strings that start at the beginning of the line with an integer and that end at the end of the line with a word.\n","3. All strings that have both the word grotto and the word raven in them (but not, e.g., words like grottos that merely contain the word grotto).\n","4. Write a pattern that captures the first word of an English sentence along with any ending punctuation if it is concatenated to it."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":468,"status":"ok","timestamp":1729444445839,"user":{"displayName":"Mohamed Hadj Ameur","userId":"13287992149799244353"},"user_tz":-60},"id":"GtfZ_0ULDCGl","outputId":"f477c649-aa20-4657-da9e-227b01dcaba0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Testing regex 1:\n","\"Humbert Humbert is a character character\" matches: Humbert Humbert\n","\"The big bug\" does not match\n","\"42 is the answer to everything\" does not match\n","\"The raven visited the grotto\" does not match\n","\"Hello, world! How are you?\" does not match\n","\n","Testing regex 2:\n","\"Humbert Humbert is a character character\" does not match\n","\"The big bug\" does not match\n","\"42 is the answer to everything\" matches: 42 is the answer to everything\n","\"The raven visited the grotto\" does not match\n","\"Hello, world! How are you?\" does not match\n","\n","Testing regex 3:\n","\"Humbert Humbert is a character character\" does not match\n","\"The big bug\" does not match\n","\"42 is the answer to everything\" does not match\n","\"The raven visited the grotto\" matches: raven visited the grotto\n","\"Hello, world! How are you?\" does not match\n","\n","Testing regex 4:\n","\"Humbert Humbert is a character character\" matches: Humbert\n","\"The big bug\" matches: The\n","\"42 is the answer to everything\" does not match\n","\"The raven visited the grotto\" matches: The\n","\"Hello, world! How are you?\" matches: Hello,\n"]}],"source":["import re\n","\n","# 1. Two consecutive repeated words\n","regex_1 = r'\\b(\\w+)\\s+\\1\\b' # \\b is the word boundry, \\1 is the first captured group\n","\n","# 2. Starts with integer, ends with word\n","regex_2 = r'^\\d+.*\\b[a-zA-Z]+\\b$'\n","\n","# 3. Contains both 'grotto' and 'raven' as whole words\n","regex_3 = r'\\b(?:grotto\\b.*\\braven|raven\\b.*\\bgrotto)\\b'\n","\n","# 4. First word of an English sentence\n","regex_4 = r'^([A-Za-z]+)[^\\w\\s]?'  # punctuation is a character that is not a word character and not a space\n","\n","# Test the regular expressions\n","test_strings = [\n","    \"Humbert Humbert is a character character\",\n","    \"The big bug\",\n","    \"42 is the answer to everything\",\n","    \"The raven visited the grotto\",\n","    \"Hello, world! How are you?\"\n","]\n","\n","for i, regex in enumerate([regex_1, regex_2, regex_3, regex_4], 1):\n","    print(f\"\\nTesting regex {i}:\")\n","    for test_str in test_strings:\n","        match = re.search(regex, test_str)\n","        if match:\n","            print(f\"\\\"{test_str}\\\" matches: {match.group()}\")\n","        else:\n","            print(f\"\\\"{test_str}\\\" does not match\")"]},{"cell_type":"markdown","metadata":{"id":"KYsHIH4lDCGn"},"source":["## Exercise 3: Regular Expression for Employee Data Extraction\n","\n","Using the provided HTML file containing employee information, write regular expressions to extract the following data: Full Name, Email Address, Phone Number, Personal Identification Code, Date of Birth, Website URL, City, Country, Job Title, Department.\n","\n","Create a regex for each data point and save the extracted employee data in a JSON file."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Employee data extracted and saved to employees_data.json.\n"]}],"source":["import re\n","import json\n","\n","# Read the HTML content from the file\n","html_content = open(\"employee-data-html.html\", \"r\", encoding='utf-8').read()\n","\n","department_pattern = r'<div class=\"department-section\" id=\"(.+?)\">.*?<h2>(.*?)</h2>.*?<div class=\"employee-grid\">(.*?)</div>\\s*</div>'\n","\n","employees_list = []\n","\n","# Find all departments and their respective employee cards\n","for department_match in re.finditer(department_pattern, html_content, re.DOTALL):\n","    department_id = department_match.group(1)  \n","    department_name = department_match.group(2)  \n","    employees_section = department_match.group(3) \n","\n","    # regex patterns for employee cards\n","    employee_pattern = r'<div class=\"employee-card\">\\s*<h3>(.+?)</h3>\\s*' \\\n","                       r'<p>Nom:\\s*<span class=\"firstname\">(.+?)</span>\\s*<span class=\"lastname\">(.+?)</span></p>\\s*' \\\n","                       r'<p>Email:\\s*(.+?)</p>\\s*' \\\n","                       r'<p>Téléphone:\\s*(.+?)</p>\\s*' \\\n","                       r'<p>Code d\\'identification:\\s*(.+?)</p>\\s*' \\\n","                       r'<p>Date de naissance:\\s*(.+?)</p>\\s*' \\\n","                       r'<p>Site Web:\\s*(.+?)</p>\\s*' \\\n","                       r'<p>Ville:\\s*(.+?),\\s*(.+?)</p>'\n","\n","    # Extract employee data from the section\n","    for employee_match in re.finditer(employee_pattern, employees_section, re.DOTALL):\n","        job_title = employee_match.group(1)\n","        first_name = employee_match.group(2)\n","        last_name = employee_match.group(3)\n","        email = employee_match.group(4)\n","        phone = employee_match.group(5)\n","        pid = employee_match.group(6)\n","        dob = employee_match.group(7)\n","        website = employee_match.group(8)\n","        city = employee_match.group(9)\n","        country = employee_match.group(10)\n","\n","        # Create a dictionary for the employee\n","        employee_data = {\n","            \"job_title\": job_title,\n","            \"full_name\": f\"{first_name} {last_name}\", \n","            \"email\": email,\n","            \"phone\": phone,\n","            \"pid\": pid,\n","            \"dob\": dob,\n","            \"website\": website,\n","            \"city\": city,\n","            \"country\": country,\n","            \"department\": department_name  \n","        }\n","        \n","        # Add employee data to the list\n","        employees_list.append(employee_data)\n","\n","# Save extracted data to a JSON file\n","with open(\"employees_data.json\", \"w\", encoding='utf-8') as json_file:\n","    json.dump(employees_list, json_file, ensure_ascii=False, indent=4)  \n","\n","print(\"Employee data extracted and saved to employees_data.json.\")\n"]},{"cell_type":"markdown","metadata":{"id":"dmdAqbmWDCGp"},"source":["## Exercise 4: Create a Chatbot Inspired by ELIZA\n","\n","Design a simple chatbot that mimics the style of ELIZA, an early AI program created in 1966 by Joseph Weizenbaum. ELIZA simulated a conversation with a psychotherapist by using pattern matching and simple responses."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMIQN49GDCGq","outputId":"317c5419-73a6-483f-b469-11003a861511"},"outputs":[{"name":"stdout","output_type":"stream","text":["ELIZA: Hello! I'm your virtual assistant. Type 'Quit' to exit.\n","ELIZA: Hello! How can I assist you today?\n","ELIZA: Stories about can are always interesting. Can you tell me more? Stories about you are always interesting. Can you tell me more? Stories about help are always interesting. Can you tell me more? Stories about me are always interesting. Can you tell me more??\n","ELIZA: Hello! How can I assist you today?\n","ELIZA: Stories about i are always interesting. Can you tell me more? Stories about am are always interesting. Can you tell me more? Stories about tired are always interesting. Can you tell me more?\n","ELIZA: Stories about tired are always interesting. Can you tell me more? Stories about and are always interesting. Can you tell me more? Stories about tired are always interesting. Can you tell me more? Stories about and are always interesting. Can you tell me more? Stories about tired are always interesting. Can you tell me more?\n","ELIZA: Stories about she are always interesting. Can you tell me more? Stories about is are always interesting. Can you tell me more? Stories about bored are always interesting. Can you tell me more?\n","ELIZA: Goodbye! It was nice talking to you.\n"]}],"source":["import re\n","\n","# Enhanced ELIZA-like chatbot\n","class ElizaChatbot:\n","    def __init__(self):\n","        self.response_rules = [\n","            (r'Quit', r'Goodbye! It was nice talking to you.'),\n","            (r'hi|hello|hey', r'Hello! How can I assist you today?'),\n","            (r'how are you\\??', r'I am just a computer program, but thanks for asking! How about you?'),\n","            (r'(\\w+) (tired|bored|sad)', r'Why do you feel \\2?'),\n","            (r'why (.*)', r'Why do you ask that?'),\n","            (r'(\\w+)', r'Stories about \\1 are always interesting. Can you tell me more?'),\n","            (r'(.*) (feel|feels) (.*)', r'What makes you feel \\3?'),\n","            (r'I (.*)', r'Why do you think you \\1?'),\n","            (r'(.*)', r'I didn’t quite understand that. Can you elaborate?'),\n","        ]\n","\n","    def eliza_response(self, user_input):\n","        # Check for specific patterns with case insensitivity\n","        for pattern, response in self.response_rules:\n","            match = re.match(pattern, user_input, re.IGNORECASE)\n","            if match:\n","                return re.sub(pattern, response, user_input, flags=re.IGNORECASE)\n","\n","        # Return a default response if no patterns matched\n","        return \"I didn't quite understand that. Can you elaborate?\"\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    eliza = ElizaChatbot()\n","    print(\"ELIZA: Hello! I'm your virtual assistant. Type 'Quit' to exit.\")\n","\n","    while True:\n","        user_input = input(\"You: \")\n","        if user_input.lower() == 'quit':\n","            print(\"ELIZA: Goodbye! It was nice talking to you.\")\n","            break\n","        print(\"ELIZA:\", eliza.eliza_response(user_input))\n"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ELIZA: Hello! I'm your empathetic listener. Type 'Quit' to exit.\n","ELIZA: Hello! I’m here to listen. What’s on your mind?\n","ELIZA: It’s okay to feel that way. Sometimes, we all have moments when we don't feel like doing things. What do you think is causing that?\n","ELIZA: Goodbye! I’m here for you anytime you need to talk.\n"]}],"source":["import re\n","\n","# Empathetic ELIZA-like chatbot for listening\n","class ListeningElizaChatbot:\n","    def __init__(self):\n","        self.response_rules = [\n","            (r'quit', r'Goodbye! I’m here for you anytime you need to talk.'),\n","            (r'hi|hello|hey', r'Hello! I’m here to listen. What’s on your mind?'),\n","            (r'how are you\\??', r'I’m just a program, but I’m here to listen to you. How are you feeling?'),\n","            (r'(\\w+) (tired|bored|sad|angry|frustrated)', r'I can sense that you’re feeling \\2. That sounds tough.'),\n","            (r'I (don\\'?t|do not) (want to) (eat|talk)', r\"It’s okay to feel that way. Sometimes, we all have moments when we don't feel like doing things. What do you think is causing that?\"),\n","            (r'why (.*)', r'It’s understandable to wonder about that. I’m here to listen.'),\n","            (r'I (.*)', r'It sounds like you’re feeling something about that: \\1. Would you like to share more?'),\n","            (r'(.*) (feel|feels) (.*)', r'It’s okay to feel \\3. I’m here for you.'),\n","            (r'(\\w+)', r'Thank you for sharing that. What else is on your mind?'),\n","            (r'(.*)', r'I’m here to listen. Please tell me more about what you’re feeling.'),\n","        ]\n","\n","    def eliza_response(self, user_input):\n","        # Check for specific patterns with case insensitivity\n","        for pattern, response in self.response_rules:\n","            match = re.match(pattern, user_input, re.IGNORECASE)\n","            if match:\n","                return re.sub(pattern, response, user_input, flags=re.IGNORECASE)\n","\n","        # Return a default response if no patterns matched\n","        return \"I didn’t quite understand that, but I'm here to listen. Can you tell me more?\"\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    eliza = ListeningElizaChatbot()\n","    print(\"ELIZA: Hello! I'm your empathetic listener. Type 'Quit' to exit.\")\n","\n","    while True:\n","        user_input = input(\"You: \")\n","        if user_input.lower() == 'quit':\n","            print(\"ELIZA: Goodbye! I’m here for you anytime you need to talk.\")\n","            break\n","        print(\"ELIZA:\", eliza.eliza_response(user_input))\n"]},{"cell_type":"markdown","metadata":{"id":"rAtIAcAIDCGr"},"source":["## Exercise 5: Edit Distance\n","\n","1. Compute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of \"leda\" to \"deal\". Show your work (using the edit distance grid).\n","\n","2. Implement a minimum edit distance algorithm and use your hand-computed results to check your code."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Minimum edit distance from 'leda' to 'deal': 4\n","Edit distance grid for 'leda' to 'deal':\n","         d   e   a   l\n","   --------------------\n","  |  0 |  1 |  2 |  3 |  4\n","l |  1 |  2 |  3 |  4 |  3\n","e |  2 |  3 |  2 |  3 |  4\n","d |  3 |  2 |  3 |  4 |  5\n","a |  4 |  3 |  4 |  3 |  4\n","\n"]}],"source":["# Function to calculate the minimum edit distance between two strings\n","def edit_distance(s1, s2):\n","    m, n = len(s1), len(s2)\n","\n","    # Create a 2D array to store distances\n","    dp = [[0] * (n + 1) for _ in range(m + 1)]\n","\n","    # Initialize the first row and column\n","    for i in range(m + 1):\n","        dp[i][0] = i  # Cost of deleting characters from s1\n","    for j in range(n + 1):\n","        dp[0][j] = j  # Cost of inserting characters into s1\n","\n","    # Fill the dp array\n","    for i in range(1, m + 1):\n","        for j in range(1, n + 1):\n","            if s1[i - 1] == s2[j - 1]:\n","                # Characters match, no extra cost\n","                dp[i][j] = dp[i - 1][j - 1]\n","            else:\n","                # Minimum of insert, delete, or substitute\n","                dp[i][j] = min(\n","                    dp[i - 1][j] + 1,  # Deletion\n","                    dp[i][j - 1] + 1,  # Insertion\n","                    dp[i - 1][j - 1] + 2  # Substitution\n","                )\n","\n","    # Return the minimum edit distance\n","    return dp[m][n], dp\n","\n","# Function to print the edit distance grid\n","def print_edit_distance_grid(s1, s2, dp):\n","    print(f\"Edit distance grid for '{s1}' to '{s2}':\")\n","    print(\"    \", \"   \".join(f\"{ch}\" for ch in \" \" + s2))\n","    print(\"   \" + \"----\" * (len(s2) + 1))\n","    for i in range(len(dp)):\n","        row_label = s1[i - 1] if i > 0 else \" \"\n","        row = \" | \".join(f\"{dp[i][j]:>2}\" for j in range(len(dp[i])))\n","        print(f\"{row_label} | {row}\")\n","    print()\n","\n","# Compute and display the edit distance for \"leda\" to \"deal\"\n","source, target = \"leda\", \"deal\"\n","distance, dp_grid = edit_distance(source, target)\n","\n","# Print the edit distance result and grid\n","print(f\"Minimum edit distance from '{source}' to '{target}': {distance}\")\n","\n","# Print the grid for clarity\n","print_edit_distance_grid(source, target, dp_grid)\n"]},{"cell_type":"markdown","metadata":{"id":"ELC0PtucDKUg"},"source":["# Exercise 6: Spell Correction using NLTK\n","\n","In this exercise, we'll implement spell correction functions using NLTK and the concepts of edit distance.\n","\n","## Question 1:\n","Write a function that takes a misspelled word and returns the closest valid word from a given textual corpus using edit distance.\n","\n","**Hint:** You can use the top 5000 most frequent words from an NLTK Gutenberg text such as 'austen-emma.txt'.\n","\n","## Question 2:\n","Use the function you created in Question 1 to correct spelling errors in full sentences.\n","\n","**Hint:** You can randomly introduce spelling errors into existing sentences from the Gutenberg Corpus available in NLTK by altering long words (those with more than 5 characters) through the insertion of a random character.\n","\n","\n","## Question 3:\n","Implement the following steps to evaluate the effectiveness of your spell correction:\n","\n","1. **Select Sentences**: Choose a set of sentences from the Gutenberg Corpus available in NLTK.\n","2. **Introduce Spelling Errors**: Randomly alter some long words in each sentence to create spelling errors (through the insertion of a random character). Ensure that only words longer than 5 characters as before.\n","3. **Correct the Sentences**: Use the function from Question 1 to correct the misspelled words in each altered sentence.\n","4. **Print Results**: For each original sentence, print the altered sentence and the suggested corrections for the misspelled words.\n","5. **Calculate Precision**: Evaluate the accuracy of the corrections by calculating the precision of the spell correction method on the provided test set in the code section below.\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/wafaa/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package gutenberg to /home/wafaa/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n","[nltk_data] Downloading package words to /home/wafaa/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]},{"ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/wafaa/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/share/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m Vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 2: Load text and tokenize, filtering punctuation and converting to lowercase\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgutenberg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mausten-emma.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha()]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Get the most frequent words as the vocabulary\u001b[39;00m\n\u001b[1;32m     20\u001b[0m fdist \u001b[38;5;241m=\u001b[39m FreqDist(tokens)\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n","File \u001b[0;32m~/anaconda3/envs/env_ds/lib/python3.9/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/wafaa/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/share/nltk_data'\n    - '/home/wafaa/anaconda3/envs/env_ds/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}],"source":["import nltk\n","from nltk.corpus import gutenberg, words\n","from nltk import FreqDist\n","from nltk.tokenize import word_tokenize\n","import random\n","from sklearn.metrics import precision_score\n","import string\n","\n","# Step 1: Download necessary NLTK data\n","nltk.download('punkt')\n","nltk.download('gutenberg')\n","nltk.download('words')\n","\n","# Define the vocab size\n","Vocab_size = 5000\n","\n","# Step 2: Load text and tokenize, filtering punctuation and converting to lowercase\n","tokens = [word.lower() for word in word_tokenize(gutenberg.raw('austen-emma.txt')) if word.isalpha()]\n","# Get the most frequent words as the vocabulary\n","fdist = FreqDist(tokens)\n","vocab = set(word for word, _ in fdist.most_common(Vocab_size))\n","\n","# Question 1: Function to find the closest word using edit distance\n","def correct_spelling(word, vocab):\n","    # Find the word with minimum edit distance from the misspelled word\n","    closest_word = min(vocab, key=lambda v: edit_distance(word, v))\n","    return closest_word\n","\n","# Question 2: Function to introduce spelling errors\n","def introduce_spelling_errors(sentence, error_rate=0.3):\n","    words = sentence.split()\n","    new_sentence = []\n","    for word in words:\n","        # Only alter words with more than 5 characters\n","        if len(word) > 5 and random.random() < error_rate:\n","            pos = random.randint(0, len(word) - 1)\n","            char = random.choice(string.ascii_lowercase)\n","            # Insert a random character at a random position\n","            word = word[:pos] + char + word[pos:]\n","        new_sentence.append(word)\n","    return \" \".join(new_sentence)\n","\n","# Question 3: Correct sentences and evaluate\n","def correct_sentence(sentence, vocab):\n","    corrected_sentence = []\n","    for word in sentence.split():\n","        if word.lower() in vocab:\n","            corrected_sentence.append(word)\n","        else:\n","            corrected_word = correct_spelling(word.lower(), vocab)\n","            corrected_sentence.append(corrected_word)\n","    return \" \".join(corrected_sentence)\n","\n","# Testing on a set of sentences\n","test_sentences = [\n","    \"She was pleased to have such a friend.\",\n","    \"He thought it was a great opportunity.\",\n","    \"The sun was shining brightly in the sky.\",\n","    \"They were walking towards the market together.\",\n","    \"I hope you will come to the party.\"\n","]\n","\n","# Introduce errors and attempt correction\n","for original_sentence in test_sentences:\n","    altered_sentence = introduce_spelling_errors(original_sentence)\n","    corrected_sentence = correct_sentence(altered_sentence, vocab)\n","\n","    print(\"Original sentence:   \", original_sentence)\n","    print(\"Altered sentence:    \", altered_sentence)\n","    print(\"Corrected sentence:  \", corrected_sentence)\n","    print()\n","\n","# Calculating precision by comparing words\n","def calculate_precision(test_sentences, vocab):\n","    y_true = []\n","    y_pred = []\n","\n","    for original_sentence in test_sentences:\n","        altered_sentence = introduce_spelling_errors(original_sentence)\n","        altered_words = altered_sentence.split()\n","        corrected_words = correct_sentence(altered_sentence, vocab).split()\n","        original_words = original_sentence.split()\n","\n","        for orig_word, corr_word in zip(original_words, corrected_words):\n","            # Append to lists for scoring purposes (1 if correct, 0 if incorrect)\n","            y_true.append(1 if orig_word.lower() in vocab else 0)\n","            y_pred.append(1 if corr_word.lower() == orig_word.lower() else 0)\n","\n","    precision = precision_score(y_true, y_pred)\n","    return precision\n","\n","# Calculate and print precision of the spell correction function\n","precision = calculate_precision(test_sentences, vocab)\n","print(f\"Precision of spell correction: {precision:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"812-dMA5EZGB"},"source":["# Exercise 7: Implementing Byte Pair Encoding (BPE) Tokenization\n","\n","In this exercise, you will implement the Byte Pair Encoding (BPE) tokenization method step by step. Follow the instructions below to build your BPE tokenizer.\n","\n","### Step 1: Get Vocabulary\n","\n","Implement the function `get_vocab(sentences)` that takes a list of sentences and returns a dictionary with the counts of each token in the vocabulary.\n","\n","### Step 2: Get Stats\n","\n","Implement the function `get_stats(vocab)` that takes the vocabulary dictionary and returns a dictionary of pairs of tokens and their frequencies.\n","\n","### Step 3: Merge Vocabulary\n","\n","Implement the function `merge_vocab(vocab, pair)` that takes the vocabulary and a pair of tokens to merge. It should return the updated vocabulary.\n","\n","### Step 4: BPE\n","\n","Implement the function `bpe(sentences, num_merges)` that uses the previously defined functions to perform BPE tokenization on the provided sentences.\n","\n","### Step 5: Apply BPE\n","\n","Implement the function `apply_bpe(sentence, vocab)` that takes a sentence and the vocabulary and returns the tokenized version of the sentence using BPE.\n","\n","Once you have completed all steps, test your implementation with sample sentences to observe how the BPE tokenization works. You can adjust the number of merges to see how it affects the tokenization.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":866,"status":"ok","timestamp":1729448153100,"user":{"displayName":"Mohamed Hadj Ameur","userId":"13287992149799244353"},"user_tz":-60},"id":"vzgK1hUhFuvI","outputId":"5e55c0e3-2206-437e-9702-aab216970a4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vocabulary after BPE: defaultdict(<class 'int'>, {})\n","Learned merges: []\n","Tokenized sentence: []\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["from collections import defaultdict\n","import nltk\n","nltk.download('punkt')  # Ensure the Punkt tokenizer is downloaded\n","\n","# Step 1: Get Vocabulary\n","def get_vocab(sentences):\n","    \"\"\"\n","    Functionality:\n","    - This function processes a list of sentences and generates a vocabulary where each word\n","        is split into individual characters (with an end-of-word marker '</w>'). The vocabulary\n","        is represented as a dictionary where keys are the tokenized words and values are their\n","        frequencies.\n","    - '</w>' is added at the end of each word. This is an \"end-of-word\" marker, which helps the\n","         algorithm distinguish between the end of a word and the beginning of the next one.\n","         It's crucial for maintaining word boundaries during the merging process.\n","    -  Example:\n","    sentences = [\"low\", \"lowest\"]\n","    Output vocab: { \"l o w </w>\": 1, \"l o w e s t </w>\": 1 }\n","\n","    Input:\n","    - sentences: A list of strings (sentences or words) to be processed into the vocabulary.\n","\n","    Output:\n","    - vocab: A dictionary where the keys are tokenized versions of words (character-level BPE tokens),\n","             and the values are the frequency of each word in the input sentences.\n","    \"\"\"\n","    vocab = defaultdict(int)\n","\n","   # your code here (don't forget to toknize each sent)\n","\n","    return vocab\n","\n","# Step 2: Get Stats\n","def get_stats(vocab):\n","    \"\"\"\n","    Functionality:\n","    - This function computes the frequency of each adjacent pair of tokens (bigrams) from the vocabulary.\n","    - It identifies which pairs of tokens (characters) appear together most frequently.\n","    - The '</w>' token is treated like any other character in this process.\n","    Example:\n","    input vocab = { \"l o w </w>\": 1, \"l o w e s t </w>\": 1 }\n","    output pairs ==> pairs = { ('l', 'o'): 2, ('o', 'w'): 2, ('w', '</w>'): 1, ('w', 'e'): 1, ('e', 's'): 1, ('s', 't'): 1, ('t', '</w>'): 1 }\n","    Input:\n","    - vocab: A dictionary where keys are tokenized words (character sequences) and values are frequencies of the words.\n","\n","    Output:\n","    - pairs: A dictionary where the keys are pairs of consecutive tokens, and the values are the sum of their\n","             frequencies across all the words in the vocabulary.\n","    \"\"\"\n","    pairs = defaultdict(int)\n","\n","    # your code here\n","\n","    return pairs\n","\n","# Step 3: Merge Vocabulary\n","def merge_vocab(vocab, pair):\n","    \"\"\"\n","    Functionality:\n","    This function merges the most frequent pair of tokens in the vocabulary by replacing the pair\n","    with a single combined token. It updates the vocabulary to reflect this merge.\n","    Example:\n","        old vocab = { \"l o w </w>\": 1, \"l o w e s t </w>\": 1}\n","        freq pair = ('o', 'w')\n","        ==> new vocab = { \"l ow </w>\": 1, \"l ow e s t </w>\": 1 }\n","\n","    Input:\n","    - vocab: A dictionary where keys are tokenized words (character sequences) and values are frequencies of the words.\n","    - pair: A tuple containing two tokens (characters) that should be merged.\n","\n","    Output:\n","    - new_vocab: A new dictionary where the specified token pair is merged in each word in the vocabulary.\n","    \"\"\"\n","    new_vocab = {}\n","\n","    # your code here\n","\n","    return new_vocab\n","\n","# Step 4: BPE (Byte Pair Encoding)\n","def bpe(sentences, num_merges):\n","    \"\"\"\n","    Functionality:\n","    This function performs the BPE algorithm on a set of sentences, iteratively merging the most\n","    frequent pairs of tokens until the specified number of merges is reached.\n","\n","    Input:\n","    - sentences: A list of strings (sentences or words) to be processed into BPE tokens.\n","    - num_merges: The number of merges to perform (i.e., how many token pairs to combine).\n","\n","    Output:\n","    - vocab: The updated vocabulary after performing the BPE merges.\n","    - merges: A list of the token pairs that were merged during the BPE process.\n","    \"\"\"\n","    vocab = get_vocab(sentences)  # Initialize vocabulary by tokenizing words into characters\n","    merges = []  # To keep track of the merges performed\n","\n","    # your code here\n","\n","    return vocab, merges\n","\n","# Step 5: Apply BPE\n","def apply_bpe(sentence, merges):\n","    \"\"\"\n","    Functionality:\n","    This function tokenizes a new sentence using the learned BPE merges. It progressively applies\n","    each merge to the sentence in the order they were learned.\n","\n","    Input:\n","    - sentence: A string (word or sentence) to be tokenized using the BPE merges.\n","    - merges: A list of tuples representing the token pairs that were merged during BPE training.\n","\n","    Output:\n","    - A list of tokens representing the sentence after applying BPE tokenization.\n","    \"\"\"\n","\n","    # your code here\n","\n","    return []  # Return the final tokenized sentence as a list of tokens\n","\n","# Example Usage\n","if __name__ == \"__main__\":\n","    # Sample sentences for BPE training\n","    sentences = [\n","        \"Low\",\n","        \"lowest\",\n","        \"the\",\n","        \"this\",\n","        \"fastest\"\n","    ]\n","    # Perform BPE with a specified number of merges (e.g., 10)\n","    num_merges = 10\n","    vocab, merges = bpe(sentences, num_merges)\n","\n","    print(\"Vocabulary after BPE:\", vocab)  # Vocabulary after applying the BPE merges\n","    print(\"Learned merges:\", merges)  # List of merges learned during BPE training\n","\n","    # Apply BPE to a new sentence that wasn't part of the original sentences\n","    sentence = \"lowder in the\"\n","    tokenized_sentence = apply_bpe(sentence, merges)\n","\n","    print(\"Tokenized sentence:\", tokenized_sentence)  # Final tokenized version of the input sentence\n"]},{"cell_type":"markdown","metadata":{"id":"vYDb4XUaXOiC"},"source":["# Exercise 8: Using SentencePiece for Byte Pair Encoding (BPE) Tokenization\n","\n","In this exercise, you will learn how to use the SentencePiece library to implement Byte Pair Encoding (BPE) for tokenization. Follow the steps below to tokenize text using BPE.\n","\n","### Step 1: Install SentencePiece\n","\n","Ensure that you have the SentencePiece library installed in your Python environment. You can install it using pip:\n","\n","```python\n","!pip install sentencepiece\n","```\n","\n","### Step 2: Prepare Your Text\n","Load and merge all texts from the Gutenberg corpus available in NLTK. You can use the following code snippet to achieve this:\n","\n","### Step 3: Train the BPE Model\n","Utilize SentencePiece to train a BPE model on the merged text. Specify the vocabulary size you want to achieve during the training process. For example:\n","```python\n","import sentencepiece as spm\n","\n","# Train the SentencePiece model\n","spm.SentencePieceTrainer.Train('--input=merged_gutenberg.txt --model_prefix=m --vocab_size=5000')\n","```\n","\n","\n","### Step 4: Tokenize the Text\n","Once the model is trained, use it to tokenize your text into subword units. You can test the tokenizer with the following sample sentences:\n","\n","```python\n","# Load the trained model\n","sp = spm.SentencePieceProcessor(model_file='m.model')\n","\n","# Tokenize a sentence\n","tokens = sp.encode(sentence, out_type=str)\n","```\n","\n","### Step 5: What about Arabic Texts !\n","Try to do the same thing but this time using an Arabic text corpus."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5955,"status":"ok","timestamp":1729444700720,"user":{"displayName":"Mohamed Hadj Ameur","userId":"13287992149799244353"},"user_tz":-60},"id":"t01YTq1IXM9E","outputId":"67eccc4f-59dc-43a7-b48c-706ee5f0d0f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]}],"source":["# Step 1: Install SentencePiece\n","!pip install sentencepiece\n","\n","# Step 2: Prepare Your Text\n","import nltk\n","from nltk.corpus import gutenberg\n","\n","# Download the Gutenberg corpus if not already downloaded\n","nltk.download('gutenberg')\n","\n","# Get the list of file IDs from the Gutenberg corpus\n","file_ids = gutenberg.fileids()\n","\n","# Merge all texts into a single string\n","merged_text = \"\"\n","for file_id in file_ids:\n","    merged_text += gutenberg.raw(file_id) + \"\\n\"\n","\n","# Write the merged text to a temporary file\n","with open('merged_gutenberg.txt', 'w', encoding='utf-8') as f:\n","    f.write(merged_text)\n"]},{"cell_type":"markdown","metadata":{"id":"v_PjHsvkOjW_"},"source":["## Main differences between our simple BPE implementation and SentencePiece BPE:\n","\n","### 1. End-of-word marker:\n","- **Simple BPE:** Uses `</w>` at the end of each word.\n","- **SentencePiece:** Uses `_` (underscore) at the beginning of each word.\n","\n","**Example:**\n","- Simple BPE: `\"low</w> est</w>\"`\n","- SentencePiece: `\"_low _est\"`\n","\n","---\n","\n","### 2. Tokenization approach:\n","- **Simple BPE:** Tokenizes words first, then applies BPE.\n","- **SentencePiece:** Treats the entire input as a sequence of characters, including spaces.\n","\n","**Example:**\n","- Simple BPE: `[\"low</w>\", \"est</w>\"]`\n","- SentencePiece: `[\"\", \"l\", \"o\", \"w\", \"\", \"e\", \"s\", \"t\"]`\n","\n","---\n","\n","### 3. Handling of whitespace:\n","- **Simple BPE:** Whitespace is removed during tokenization.\n","- **SentencePiece:** Preserves whitespace as part of the tokenization process.\n","\n","**Example:**\n","- Simple BPE: `\"hello world\"` -> `[\"hello</w>\", \"world</w>\"]`\n","- SentencePiece: `\"hello world\"` -> `[\"_hello\", \"_world\"]`\n","\n","---\n","\n","### 4. Subword regularization:\n","- **Simple BPE:** No built-in regularization.\n","- **SentencePiece:** Supports subword regularization techniques like BPE-dropout.\n","\n","---\n","\n","### 5. Unicode handling:\n","- **Simple BPE:** Limited Unicode support (depends on NLTK tokenization).\n","- **SentencePiece:** Full Unicode support, treating each Unicode character as a basic unit.\n","\n","---\n","\n","### 6. Vocabulary size control:\n","- **Simple BPE:** Controlled by number of merge operations.\n","- **SentencePiece:** Directly specify desired vocabulary size.\n","\n","---\n","\n","### 7. Out-of-vocabulary (OOV) handling:\n","- **Simple BPE:** No specific OOV handling.\n","- **SentencePiece:** Has built-in OOV handling, often using a special `<unk>` token.\n","\n","---\n","\n","### 8. Reversibility:\n","- **Simple BPE:** Not easily reversible due to `</w>` placement.\n","- **SentencePiece:** Easily reversible due to `_` placement at word beginnings.\n","\n","**Example:**\n","- SentencePiece: `\"_app le\"` -> `\"apple\"` (easily reversible)\n","- Simple BPE: `\"app le</w>\"` -> `\"apple\"` (requires special handling of `</w>`)\n","\n","---\n","\n","### 9. Efficiency and speed:\n","- **Simple BPE:** Generally very slow, especially for large datasets.\n","- **SentencePiece:** Highly optimized for speed and efficiency.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env_ds","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
